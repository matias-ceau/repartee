Completion()OutputOn this pageOutputFormat​Here's the exact json output and type you can expect from all litellm completion calls for all models{  'choices': [    {      'finish_reason': str,     # String: 'stop'      'index': int,             # Integer: 0      'message': {              # Dictionary [str, str]        'role': str,            # String: 'assistant'        'content': str          # String: "default message"      }    }  ],  'created': str,               # String: None  'model': str,                 # String: None  'usage': {                    # Dictionary [str, int]    'prompt_tokens': int,       # Integer    'completion_tokens': int,   # Integer    'total_tokens': int         # Integer  }}You can access the response as a dictionary or as a class object, just as OpenAI allows youprint(response.choices[0].message.content)print(response['choices'][0]['message']['content'])Here's what an example response looks like {  'choices': [     {        'finish_reason': 'stop',        'index': 0,        'message': {           'role': 'assistant',            'content': " I'm doing well, thank you for asking. I am Claude, an AI assistant created by Anthropic."        }      }    ], 'created': 1691429984.3852863, 'model': 'claude-instant-1', 'usage': {'prompt_tokens': 18, 'completion_tokens': 23, 'total_tokens': 41}}Additional Attributes​You can also access information like latency. from litellm import completionimport osos.environ["ANTHROPIC_API_KEY"] = "your-api-key"messages=[{"role": "user", "content": "Hey!"}]response = completion(model="claude-2", messages=messages)print(response.response_ms) # 616.25# 616.25PreviousPrompt FormattingNextException MappingFormatAdditional Attributes

💥 OpenAI Proxy Server💥 OpenAI Proxy ServerProxy Server to call 100+ LLMs in a unified interface & track spend, set budgets per virtual key/user📄️ Quick StartQuick start CLI, Config, Docker📄️ 🐳 Docker, Deploying LiteLLM ProxyYou can find the Dockerfile to build litellm proxy here📄️ ⚡ Best Practices for Production1. Use this config.yaml🔗 📖 All Endpoints (Swagger)📄️ 🎉 Demo AppHere is a demo of the proxy. To log in pass in:📄️ Proxy Config.yamlSet model list, apibase, apikey, temperature & proxy server settings (master-key) on the config.yaml.📄️ 🔥 Fallbacks, Retries, Timeouts, Load BalancingRetry call with multiple instances of the same model.📄️ 💸 Spend TrackingTrack spend for keys, users, and teams across 100+ LLMs.📄️ 💰 Budgets, Rate LimitsRequirements:📄️ 🙋‍♂️ CustomersTrack spend, set budgets for your customers.📄️ 💵 BillingBill internal teams, external customers for their usage📄️ Use with Langchain, OpenAI SDK, LlamaIndex, CurlInput, Output, Exceptions are mapped to the OpenAI format for all supported models📄️ ✨ Enterprise Features - Content Mod, SSO, Custom SwaggerFeatures here are behind a commercial license in our /enterprise folder. See Code📄️ 🔑 Virtual KeysTrack Spend, and control model access via virtual keys for the proxy📄️ 🚨 Alerting / WebhooksGet alerts for:🗃️ 🪢 Logging2 items📄️ [BETA] Proxy UICreate + delete keys through a UI📄️ ✨ 📧 Email NotificationsSend an Email to your users when:📄️ 👥 Team-based Routing + LoggingRouting📄️ Region-based RoutingRoute specific customers to eu-only models.📄️ [BETA] JWT-based AuthUse JWT's to auth admins / projects into the proxy.🗃️ Extra Load Balancing1 items📄️ Model ManagementAdd new models + Get model info without restarting proxy.📄️ Health ChecksUse this to health check all LLMs defined in your config.yaml📄️ Debugging2 levels of debugging supported.📄️ PII MaskingLiteLLM supports Microsoft Presidio for PII masking.📄️ 🕵️ Prompt Injection DetectionLiteLLM Supports the following methods for detecting prompt injection attacks📄️ CachingCache LLM Responses📄️ Grafana, Prometheus metrics [BETA]LiteLLM Exposes a /metrics endpoint for Prometheus to Poll📄️ Modify / Reject Incoming Requests- Modify data before making llm api calls on proxy📄️ Post-Call RulesUse this to fail a request based on the output of an llm api call.📄️ CLI ArgumentsCli arguments,  --host, --port, --num_workersPreviousLiteLLM - Getting StartedNextQuick Start

💥 OpenAI Proxy Server🔑 Virtual KeysOn this page🔑 Virtual KeysTrack Spend, and control model access via virtual keys for the proxyinfo🔑 UI to Generate, Edit, Delete Keys (with SSO)Deploy LiteLLM Proxy with Key ManagementDockerfile.database for LiteLLM Proxy + Key ManagementSetup​Requirements: Need a postgres database (e.g. Supabase, Neon, etc)Set DATABASE_URL=postgresql://<user>:<password>@<host>:<port>/<dbname> in your env Set a master key, this is your Proxy Admin key - you can use this to create other keys (🚨 must start with sk-). Set on config.yaml set your master key under general_settings:master_key, example below Set env variable set LITELLM_MASTER_KEY(the proxy Dockerfile checks if the DATABASE_URL is set and then intializes the DB connection)export DATABASE_URL=postgresql://<user>:<password>@<host>:<port>/<dbname>You can then generate keys by hitting the /key/generate endpoint.See codeStep 1: Save postgres db urlmodel_list:  - model_name: gpt-4    litellm_params:        model: ollama/llama2  - model_name: gpt-3.5-turbo    litellm_params:        model: ollama/llama2general_settings:   master_key: sk-1234   database_url: "postgresql://<user>:<password>@<host>:<port>/<dbname>" # 👈 KEY CHANGEStep 2: Start litellmlitellm --config /path/to/config.yamlStep 3: Generate keyscurl 'http://0.0.0.0:4000/key/generate' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4"], "metadata": {"user": "ishaan@berri.ai"}}'Advanced - Spend Tracking​Get spend per:key - via /key/info Swaggeruser - via /user/info Swaggerteam - via /team/info Swagger ⏳ end-users - via /end_user/info - Comment on this issue for end-user cost trackingHow is it calculated?The cost per model is stored here and calculated by the completion_cost function.How is it tracking?Spend is automatically tracked for the key in the "LiteLLM_VerificationTokenTable". If the key has an attached 'user_id' or 'team_id', the spend for that user is tracked in the "LiteLLM_UserTable", and team in the "LiteLLM_TeamTable".Key SpendUser SpendTeam SpendYou can get spend for a key by using the /key/info endpoint. curl 'http://0.0.0.0:4000/key/info?key=<user-key>' \     -X GET \     -H 'Authorization: Bearer <your-master-key>'This is automatically updated (in USD) when calls are made to /completions, /chat/completions, /embeddings using litellm's completion_cost() function. See Code. Sample response{    "key": "sk-tXL0wt5-lOOVK9sfY2UacA",    "info": {        "token": "sk-tXL0wt5-lOOVK9sfY2UacA",        "spend": 0.0001065, # 👈 SPEND        "expires": "2023-11-24T23:19:11.131000Z",        "models": [            "gpt-3.5-turbo",            "gpt-4",            "claude-2"        ],        "aliases": {            "mistral-7b": "gpt-3.5-turbo"        },        "config": {}    }}1. Create a usercurl --location 'http://localhost:4000/user/new' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{user_email: "krrish@berri.ai"}' Expected Response{    ...    "expires": "2023-12-22T09:53:13.861000Z",    "user_id": "my-unique-id", # 👈 unique id    "max_budget": 0.0}2. Create a key for that usercurl 'http://0.0.0.0:4000/key/generate' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4"], "user_id": "my-unique-id"}'Returns a key - sk-....3. See spend for usercurl 'http://0.0.0.0:4000/user/info?user_id=my-unique-id' \     -X GET \     -H 'Authorization: Bearer <your-master-key>'Expected Response{  ...  "spend": 0 # 👈 SPEND}Use teams, if you want keys to be owned by multiple people (e.g. for a production app).1. Create a teamcurl --location 'http://localhost:4000/team/new' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{"team_alias": "my-awesome-team"}' Expected Response{    ...    "expires": "2023-12-22T09:53:13.861000Z",    "team_id": "my-unique-id", # 👈 unique id    "max_budget": 0.0}2. Create a key for that teamcurl 'http://0.0.0.0:4000/key/generate' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4"], "team_id": "my-unique-id"}'Returns a key - sk-....3. See spend for teamcurl 'http://0.0.0.0:4000/team/info?team_id=my-unique-id' \     -X GET \     -H 'Authorization: Bearer <your-master-key>'Expected Response{  ...  "spend": 0 # 👈 SPEND}Advanced - Model Access​Restrict models by team_id​litellm-dev can only access azure-gpt-3.51. Create a team via /team/newcurl --location 'http://localhost:4000/team/new' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{  "team_alias": "litellm-dev",  "models": ["azure-gpt-3.5"]}' # returns {...,"team_id": "my-unique-id"}2. Create a key for teamcurl --location 'http://localhost:4000/key/generate' \--header 'Authorization: Bearer sk-1234' \--header 'Content-Type: application/json' \--data-raw '{"team_id": "my-unique-id"}'3. Test itcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --header 'Authorization: Bearer sk-qo992IjKOC2CHKZGRoJIGA' \    --data '{        "model": "BEDROCK_GROUP",        "messages": [            {                "role": "user",                "content": "hi"            }        ]    }'{"error":{"message":"Invalid model for team litellm-dev: BEDROCK_GROUP.  Valid models for team are: ['azure-gpt-3.5']\n\n\nTraceback (most recent call last):\n  File \"/Users/ishaanjaffer/Github/litellm/litellm/proxy/proxy_server.py\", line 2298, in chat_completion\n    _is_valid_team_configs(\n  File \"/Users/ishaanjaffer/Github/litellm/litellm/proxy/utils.py\", line 1296, in _is_valid_team_configs\n    raise Exception(\nException: Invalid model for team litellm-dev: BEDROCK_GROUP.  Valid models for team are: ['azure-gpt-3.5']\n\n","type":"None","param":"None","code":500}}%            Model Aliases​If a user is expected to use a given model (i.e. gpt3-5), and you want to:try to upgrade the request (i.e. GPT4)or downgrade it (i.e. Mistral)OR rotate the API KEY (i.e. open AI)OR access the same model through different end points (i.e. openAI vs openrouter vs Azure)Here's how you can do that: Step 1: Create a model group in config.yaml (save model name, api keys, etc.)model_list:  - model_name: my-free-tier    litellm_params:        model: huggingface/HuggingFaceH4/zephyr-7b-beta        api_base: http://0.0.0.0:8001  - model_name: my-free-tier    litellm_params:        model: huggingface/HuggingFaceH4/zephyr-7b-beta        api_base: http://0.0.0.0:8002  - model_name: my-free-tier    litellm_params:        model: huggingface/HuggingFaceH4/zephyr-7b-beta        api_base: http://0.0.0.0:8003    - model_name: my-paid-tier    litellm_params:        model: gpt-4        api_key: my-api-keyStep 2: Generate a user key - enabling them access to specific models, custom model aliases, etc.curl -X POST "https://0.0.0.0:4000/key/generate" \-H "Authorization: Bearer <your-master-key>" \-H "Content-Type: application/json" \-d '{    "models": ["my-free-tier"],     "aliases": {"gpt-3.5-turbo": "my-free-tier"},     "duration": "30min"}'How to upgrade / downgrade request? Change the alias mappingHow are routing between diff keys/api bases done? litellm handles this by shuffling between different models in the model list with the same model_name. See CodeGrant Access to new model​Use model access groups to give users access to select models, and add new ones to it over time (e.g. mistral, llama-2, etc.)Step 1. Assign model, access group in config.yamlmodel_list:  - model_name: text-embedding-ada-002    litellm_params:      model: azure/azure-embedding-model      api_base: "os.environ/AZURE_API_BASE"      api_key: "os.environ/AZURE_API_KEY"      api_version: "2023-07-01-preview"    model_info:      access_groups: ["beta-models"] # 👈 Model Access GroupStep 2. Create key with access groupcurl --location 'http://localhost:4000/key/generate' \-H 'Authorization: Bearer <your-master-key>' \-H 'Content-Type: application/json' \-d '{"models": ["beta-models"], # 👈 Model Access Group            "max_budget": 0,}'Advanced - Custom Auth​You can now override the default api key auth.Here's how: 1. Create a custom auth file.​Make sure the response type follows the UserAPIKeyAuth pydantic object. This is used by for logging usage specific to that user key.from litellm.proxy._types import UserAPIKeyAuthasync def user_api_key_auth(request: Request, api_key: str) -> UserAPIKeyAuth:     try:         modified_master_key = "sk-my-master-key"        if api_key == modified_master_key:            return UserAPIKeyAuth(api_key=api_key)        raise Exception    except:         raise Exception2. Pass the filepath (relative to the config.yaml)​Pass the filepath to the config.yaml e.g. if they're both in the same dir - ./config.yaml and ./custom_auth.py, this is what it looks like:model_list:   - model_name: "openai-model"    litellm_params:       model: "gpt-3.5-turbo"litellm_settings:  drop_params: True  set_verbose: Truegeneral_settings:  custom_auth: custom_auth.user_api_key_authImplementation Code3. Start the proxy​$ litellm --config /path/to/config.yaml Custom /key/generate​If you need to add custom logic before generating a Proxy API Key (Example Validating team_id)1. Write a custom custom_generate_key_fn​The input to the custom_generate_key_fn function is a single parameter: data (Type: GenerateKeyRequest)The output of your custom_generate_key_fn should be a dictionary with the following structure{    "decision": False,    "message": "This violates LiteLLM Proxy Rules. No team id provided.",}decision (Type: bool): A boolean value indicating whether the key generation is allowed (True) or not (False).message (Type: str, Optional): An optional message providing additional information about the decision. This field is included when the decision is False.async def custom_generate_key_fn(data: GenerateKeyRequest)-> dict:        """        Asynchronous function for generating a key based on the input data.        Args:            data (GenerateKeyRequest): The input data for key generation.        Returns:            dict: A dictionary containing the decision and an optional message.            {                "decision": False,                "message": "This violates LiteLLM Proxy Rules. No team id provided.",            }        """                # decide if a key should be generated or not        print("using custom auth function!")        data_json = data.json()  # type: ignore        # Unpacking variables        team_id = data_json.get("team_id")        duration = data_json.get("duration")        models = data_json.get("models")        aliases = data_json.get("aliases")        config = data_json.get("config")        spend = data_json.get("spend")        user_id = data_json.get("user_id")        max_parallel_requests = data_json.get("max_parallel_requests")        metadata = data_json.get("metadata")        tpm_limit = data_json.get("tpm_limit")        rpm_limit = data_json.get("rpm_limit")        if team_id is not None and team_id == "litellm-core-infra@gmail.com":            # only team_id="litellm-core-infra@gmail.com" can make keys            return {                "decision": True,            }        else:            print("Failed custom auth")            return {                "decision": False,                "message": "This violates LiteLLM Proxy Rules. No team id provided.",            }2. Pass the filepath (relative to the config.yaml)​Pass the filepath to the config.yaml e.g. if they're both in the same dir - ./config.yaml and ./custom_auth.py, this is what it looks like:model_list:   - model_name: "openai-model"    litellm_params:       model: "gpt-3.5-turbo"litellm_settings:  drop_params: True  set_verbose: Truegeneral_settings:  custom_key_generate: custom_auth.custom_generate_key_fnUpperbound /key/generate params​Use this, if you need to set default upperbounds for max_budget, budget_duration or any key/generate param per key. Set litellm_settings:upperbound_key_generate_params:litellm_settings:  upperbound_key_generate_params:    max_budget: 100 # upperbound of $100, for all /key/generate requests    duration: "30d" # upperbound of 30 days for all /key/generate requests Expected Behavior Send a /key/generate request with max_budget=200Key will be created with max_budget=100 since 100 is the upper boundDefault /key/generate params​Use this, if you need to control the default max_budget or any key/generate param per key. When a /key/generate request does not specify max_budget, it will use the max_budget specified in default_key_generate_paramsSet litellm_settings:default_key_generate_params:litellm_settings:  default_key_generate_params:    max_budget: 1.5000    models: ["azure-gpt-3.5"]    duration:     # blank means `null`    metadata: {"setting":"default"}    team_id: "core-infra"Endpoints​Keys​👉 API REFERENCE DOCS​Users​👉 API REFERENCE DOCS​Teams​👉 API REFERENCE DOCS​Previous✨ Enterprise Features - Content Mod, SSO, Custom SwaggerNext🚨 Alerting / WebhooksSetupAdvanced - Spend TrackingAdvanced - Model AccessRestrict models by team_idModel AliasesGrant Access to new modelAdvanced - Custom AuthCustom /key/generateUpperbound /key/generate paramsDefault /key/generate paramsEndpointsKeysUsersTeams

💥 OpenAI Proxy Server🪢 Logging🪢 Logging - Langfuse, OpenTelemetry, Custom Callbacks, DataDog, s3 Bucket, Sentry, Athina, Azure Content-SafetyOn this page🪢 Logging - Langfuse, OpenTelemetry, Custom Callbacks, DataDog, s3 Bucket, Sentry, Athina, Azure Content-SafetyLog Proxy Input, Output, Exceptions using Langfuse, OpenTelemetry, Custom Callbacks, DataDog, DynamoDB, s3 BucketLogging to LangfuseLogging with OpenTelemetry (OpenTelemetry)Async Custom CallbacksAsync Custom Callback APIsLogging to OpenMeterLogging to s3 BucketsLogging to DataDogLogging to DynamoDBLogging to SentryLogging to Athina(BETA) Moderation with Azure Content-SafetyLogging Proxy Input/Output - Langfuse​We will use the --config to set litellm.success_callback = ["langfuse"] this will log all successfull LLM calls to langfuse. Make sure to set LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY in your environmentStep 1 Install langfusepip install langfuse>=2.0.0Step 2: Create a config.yaml file and set litellm_settings: success_callbackmodel_list: - model_name: gpt-3.5-turbo    litellm_params:      model: gpt-3.5-turbolitellm_settings:  success_callback: ["langfuse"]Step 3: Set required env variables for logging to langfuseexport LANGFUSE_PUBLIC_KEY="pk_kk"export LANGFUSE_SECRET_KEY="sk_ssStep 4: Start the proxy, make a test requestStart proxylitellm --config config.yaml --debugTest Requestlitellm --testExpected output on LangfuseLogging Metadata to Langfuse​Curl RequestOpenAI v1.0.0+LangchainPass metadata as part of the request bodycurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --data '{    "model": "gpt-3.5-turbo",    "messages": [        {        "role": "user",        "content": "what llm are you"        }    ],    "metadata": {        "generation_name": "ishaan-test-generation",        "generation_id": "gen-id22",        "trace_id": "trace-id22",        "trace_user_id": "user-id2"    }}'Set extra_body={"metadata": { }} to metadata you want to passimport openaiclient = openai.OpenAI(    api_key="anything",    base_url="http://0.0.0.0:4000")# request sent to model set on litellm proxy, `litellm --model`response = client.chat.completions.create(    model="gpt-3.5-turbo",    messages = [        {            "role": "user",            "content": "this is a test request, write a short poem"        }    ],    extra_body={        "metadata": {            "generation_name": "ishaan-generation-openai-client",            "generation_id": "openai-client-gen-id22",            "trace_id": "openai-client-trace-id22",            "trace_user_id": "openai-client-user-id2"        }    })print(response)from langchain.chat_models import ChatOpenAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    HumanMessagePromptTemplate,    SystemMessagePromptTemplate,)from langchain.schema import HumanMessage, SystemMessagechat = ChatOpenAI(    openai_api_base="http://0.0.0.0:4000",    model = "gpt-3.5-turbo",    temperature=0.1,    extra_body={        "metadata": {            "generation_name": "ishaan-generation-langchain-client",            "generation_id": "langchain-client-gen-id22",            "trace_id": "langchain-client-trace-id22",            "trace_user_id": "langchain-client-user-id2"        }    })messages = [    SystemMessage(        content="You are a helpful assistant that im using to make a test request to."    ),    HumanMessage(        content="test from litellm. tell me why it's amazing in 1 sentence"    ),]response = chat(messages)print(response)Team based Logging to Langfuse​Example:This config would send langfuse logs to 2 different langfuse projects, based on the team id litellm_settings:  default_team_settings:     - team_id: my-secret-project      success_callback: ["langfuse"]      langfuse_public_key: os.environ/LANGFUSE_PUB_KEY_1 # Project 1      langfuse_secret: os.environ/LANGFUSE_PRIVATE_KEY_1 # Project 1    - team_id: ishaans-secret-project      success_callback: ["langfuse"]      langfuse_public_key: os.environ/LANGFUSE_PUB_KEY_2 # Project 2      langfuse_secret: os.environ/LANGFUSE_SECRET_2 # Project 2Now, when you generate keys for this team-id curl -X POST 'http://0.0.0.0:4000/key/generate' \-H 'Authorization: Bearer sk-1234' \-H 'Content-Type: application/json' \-d '{"team_id": "ishaans-secret-project"}'All requests made with these keys will log data to their team-specific logging.Redacting Messages, Response Content from Langfuse Logging​Set litellm.turn_off_message_logging=True This will prevent the messages and responses from being logged to langfuse, but request metadata will still be logged.model_list: - model_name: gpt-3.5-turbo    litellm_params:      model: gpt-3.5-turbolitellm_settings:  success_callback: ["langfuse"]  turn_off_message_logging: True🔧 Debugging - Viewing RAW CURL sent from LiteLLM to provider​Use this when you want to view the RAW curl request sent from LiteLLM to the LLM API Curl RequestOpenAI v1.0.0+LangchainPass metadata as part of the request bodycurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --data '{    "model": "gpt-3.5-turbo",    "messages": [        {        "role": "user",        "content": "what llm are you"        }    ],    "metadata": {        "log_raw_request": true    }}'Set extra_body={"metadata": {"log_raw_request": True }} to metadata you want to passimport openaiclient = openai.OpenAI(    api_key="anything",    base_url="http://0.0.0.0:4000")# request sent to model set on litellm proxy, `litellm --model`response = client.chat.completions.create(    model="gpt-3.5-turbo",    messages = [        {            "role": "user",            "content": "this is a test request, write a short poem"        }    ],    extra_body={        "metadata": {            "log_raw_request": True        }    })print(response)from langchain.chat_models import ChatOpenAIfrom langchain.prompts.chat import (    ChatPromptTemplate,    HumanMessagePromptTemplate,    SystemMessagePromptTemplate,)from langchain.schema import HumanMessage, SystemMessagechat = ChatOpenAI(    openai_api_base="http://0.0.0.0:4000",    model = "gpt-3.5-turbo",    temperature=0.1,    extra_body={        "metadata": {            "log_raw_request": True        }    })messages = [    SystemMessage(        content="You are a helpful assistant that im using to make a test request to."    ),    HumanMessage(        content="test from litellm. tell me why it's amazing in 1 sentence"    ),]response = chat(messages)print(response)Expected Output on LangfuseYou will see raw_request in your Langfuse Metadata. This is the RAW CURL command sent from LiteLLM to your LLM API providerLogging Proxy Input/Output in OpenTelemetry format​Log to consoleLog to HoneycombLog to OTEL HTTP CollectorLog to OTEL GRPC CollectorLog to Traceloop CloudStep 1: Set callbacks and env varsAdd the following to your envOTEL_EXPORTER="console"Add otel as a callback on your litellm_config.yamllitellm_settings:  callbacks: ["otel"]Step 2: Start the proxy, make a test requestStart proxylitellm --config config.yaml --detailed_debugTest Requestcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --data ' {    "model": "gpt-3.5-turbo",    "messages": [        {        "role": "user",        "content": "what llm are you"        }    ]    }'Step 3: Expect to see the following logged on your server logs / consoleThis is the Span from OTEL Logging{    "name": "litellm-acompletion",    "context": {        "trace_id": "0x8d354e2346060032703637a0843b20a3",        "span_id": "0xd8d3476a2eb12724",        "trace_state": "[]"    },    "kind": "SpanKind.INTERNAL",    "parent_id": null,    "start_time": "2024-06-04T19:46:56.415888Z",    "end_time": "2024-06-04T19:46:56.790278Z",    "status": {        "status_code": "OK"    },    "attributes": {        "model": "llama3-8b-8192"    },    "events": [],    "links": [],    "resource": {        "attributes": {            "service.name": "litellm"        },        "schema_url": ""    }}Quick Start - Log to Honeycomb​Step 1: Set callbacks and env varsAdd the following to your envOTEL_EXPORTER="otlp_http"OTEL_ENDPOINT="https://api.honeycomb.io/v1/traces"OTEL_HEADERS="x-honeycomb-team=<your-api-key>"Add otel as a callback on your litellm_config.yamllitellm_settings:  callbacks: ["otel"]Step 2: Start the proxy, make a test requestStart proxylitellm --config config.yaml --detailed_debugTest Requestcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --data ' {    "model": "gpt-3.5-turbo",    "messages": [        {        "role": "user",        "content": "what llm are you"        }    ]    }'Quick Start - Log to OTEL Collector​Step 1: Set callbacks and env varsAdd the following to your envOTEL_EXPORTER="otlp_http"OTEL_ENDPOINT="http:/0.0.0.0:4317"OTEL_HEADERS="x-honeycomb-team=<your-api-key>" # OptionalAdd otel as a callback on your litellm_config.yamllitellm_settings:  callbacks: ["otel"]Step 2: Start the proxy, make a test requestStart proxylitellm --config config.yaml --detailed_debugTest Requestcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --data ' {    "model": "gpt-3.5-turbo",    "messages": [        {        "role": "user",        "content": "what llm are you"        }    ]    }'Quick Start - Log to OTEL GRPC Collector​Step 1: Set callbacks and env varsAdd the following to your envOTEL_EXPORTER="otlp_grpc"OTEL_ENDPOINT="http:/0.0.0.0:4317"OTEL_HEADERS="x-honeycomb-team=<your-api-key>" # OptionalAdd otel as a callback on your litellm_config.yamllitellm_settings:  callbacks: ["otel"]Step 2: Start the proxy, make a test requestStart proxylitellm --config config.yaml --detailed_debugTest Requestcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --data ' {    "model": "gpt-3.5-turbo",    "messages": [        {        "role": "user",        "content": "what llm are you"        }    ]    }'Quick Start - Log to Traceloop​Step 1: Install the traceloop-sdk SDKpip install traceloop-sdk==0.21.2Step 2: Add traceloop as a success_callbacklitellm_settings:  success_callback: ["traceloop"]environment_variables:  TRACELOOP_API_KEY: "XXXXX"Step 3: Start the proxy, make a test requestStart proxylitellm --config config.yaml --detailed_debugTest Requestcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --data ' {    "model": "gpt-3.5-turbo",    "messages": [        {        "role": "user",        "content": "what llm are you"        }    ]    }' 🎉 Expect to see this trace logged in your OTEL collectorCustom Callback Class [Async]​Use this when you want to run custom callbacks in pythonStep 1 - Create your custom litellm callback class​We use litellm.integrations.custom_logger for this, more details about litellm custom callbacks hereDefine your custom callback class in a python file.Here's an example custom logger for tracking key, user, model, prompt, response, tokens, cost. We create a file called custom_callbacks.py and initialize proxy_handler_instance from litellm.integrations.custom_logger import CustomLoggerimport litellm# This file includes the custom callbacks for LiteLLM Proxy# Once defined, these can be passed in proxy_config.yamlclass MyCustomHandler(CustomLogger):    def log_pre_api_call(self, model, messages, kwargs):         print(f"Pre-API Call")        def log_post_api_call(self, kwargs, response_obj, start_time, end_time):         print(f"Post-API Call")    def log_stream_event(self, kwargs, response_obj, start_time, end_time):        print(f"On Stream")            def log_success_event(self, kwargs, response_obj, start_time, end_time):         print("On Success")    def log_failure_event(self, kwargs, response_obj, start_time, end_time):         print(f"On Failure")    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):        print(f"On Async Success!")        # log: key, user, model, prompt, response, tokens, cost        # Access kwargs passed to litellm.completion()        model = kwargs.get("model", None)        messages = kwargs.get("messages", None)        user = kwargs.get("user", None)        # Access litellm_params passed to litellm.completion(), example access `metadata`        litellm_params = kwargs.get("litellm_params", {})        metadata = litellm_params.get("metadata", {})   # headers passed to LiteLLM proxy, can be found here        # Calculate cost using  litellm.completion_cost()        cost = litellm.completion_cost(completion_response=response_obj)        response = response_obj        # tokens used in response         usage = response_obj["usage"]        print(            f"""                Model: {model},                Messages: {messages},                User: {user},                Usage: {usage},                Cost: {cost},                Response: {response}                Proxy Metadata: {metadata}            """        )        return    async def async_log_failure_event(self, kwargs, response_obj, start_time, end_time):         try:            print(f"On Async Failure !")            print("\nkwargs", kwargs)            # Access kwargs passed to litellm.completion()            model = kwargs.get("model", None)            messages = kwargs.get("messages", None)            user = kwargs.get("user", None)            # Access litellm_params passed to litellm.completion(), example access `metadata`            litellm_params = kwargs.get("litellm_params", {})            metadata = litellm_params.get("metadata", {})   # headers passed to LiteLLM proxy, can be found here            # Acess Exceptions & Traceback            exception_event = kwargs.get("exception", None)            traceback_event = kwargs.get("traceback_exception", None)            # Calculate cost using  litellm.completion_cost()            cost = litellm.completion_cost(completion_response=response_obj)            print("now checking response obj")                        print(                f"""                    Model: {model},                    Messages: {messages},                    User: {user},                    Cost: {cost},                    Response: {response_obj}                    Proxy Metadata: {metadata}                    Exception: {exception_event}                    Traceback: {traceback_event}                """            )        except Exception as e:            print(f"Exception: {e}")proxy_handler_instance = MyCustomHandler()# Set litellm.callbacks = [proxy_handler_instance] on the proxy# need to set litellm.callbacks = [proxy_handler_instance] # on the proxyStep 2 - Pass your custom callback class in config.yaml​We pass the custom callback class defined in Step1 to the config.yaml.
Set callbacks to python_filename.logger_instance_nameIn the config below, we passpython_filename: custom_callbacks.pylogger_instance_name: proxy_handler_instance. This is defined in Step 1callbacks: custom_callbacks.proxy_handler_instancemodel_list:  - model_name: gpt-3.5-turbo    litellm_params:      model: gpt-3.5-turbolitellm_settings:  callbacks: custom_callbacks.proxy_handler_instance # sets litellm.callbacks = [proxy_handler_instance]Step 3 - Start proxy + test request​litellm --config proxy_config.yamlcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Authorization: Bearer sk-1234' \    --data ' {    "model": "gpt-3.5-turbo",    "messages": [        {        "role": "user",        "content": "good morning good sir"        }    ],    "user": "ishaan-app",    "temperature": 0.2    }'Resulting Log on Proxy​On Success    Model: gpt-3.5-turbo,    Messages: [{'role': 'user', 'content': 'good morning good sir'}],    User: ishaan-app,    Usage: {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21},    Cost: 3.65e-05,    Response: {'id': 'chatcmpl-8S8avKJ1aVBg941y5xzGMSKrYCMvN', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': 'Good morning! How can I assist you today?', 'role': 'assistant'}}], 'created': 1701716913, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21}}    Proxy Metadata: {'user_api_key': None, 'headers': Headers({'host': '0.0.0.0:4000', 'user-agent': 'curl/7.88.1', 'accept': '*/*', 'authorization': 'Bearer sk-1234', 'content-length': '199', 'content-type': 'application/x-www-form-urlencoded'}), 'model_group': 'gpt-3.5-turbo', 'deployment': 'gpt-3.5-turbo-ModelID-gpt-3.5-turbo'}Logging Proxy Request Object, Header, Url​Here's how you can access the url, headers, request body sent to the proxy for each requestclass MyCustomHandler(CustomLogger):    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):        print(f"On Async Success!")        litellm_params = kwargs.get("litellm_params", None)        proxy_server_request = litellm_params.get("proxy_server_request")        print(proxy_server_request)Expected Output{  "url": "http://testserver/chat/completions",  "method": "POST",  "headers": {    "host": "testserver",    "accept": "*/*",    "accept-encoding": "gzip, deflate",    "connection": "keep-alive",    "user-agent": "testclient",    "authorization": "Bearer None",    "content-length": "105",    "content-type": "application/json"  },  "body": {    "model": "Azure OpenAI GPT-4 Canada",    "messages": [      {        "role": "user",        "content": "hi"      }    ],    "max_tokens": 10  }}Logging model_info set in config.yaml​Here is how to log the model_info set in your proxy config.yaml. Information on setting model_info on config.yamlclass MyCustomHandler(CustomLogger):    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):        print(f"On Async Success!")        litellm_params = kwargs.get("litellm_params", None)        model_info = litellm_params.get("model_info")        print(model_info)Expected Output{'mode': 'embedding', 'input_cost_per_token': 0.002}Logging responses from proxy​Both /chat/completions and /embeddings responses are available as response_objNote: for /chat/completions, both stream=True and non stream responses are available as response_objclass MyCustomHandler(CustomLogger):    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):        print(f"On Async Success!")        print(response_obj)Expected Output /chat/completion [for both stream and non-stream responses]ModelResponse(    id='chatcmpl-8Tfu8GoMElwOZuj2JlHBhNHG01PPo',    choices=[        Choices(            finish_reason='stop',            index=0,            message=Message(                content='As an AI language model, I do not have a physical body and therefore do not possess any degree or educational qualifications. My knowledge and abilities come from the programming and algorithms that have been developed by my creators.',                role='assistant'            )        )    ],    created=1702083284,    model='chatgpt-v-2',    object='chat.completion',    system_fingerprint=None,    usage=Usage(        completion_tokens=42,        prompt_tokens=5,        total_tokens=47    ))Expected Output /embeddings{    'model': 'ada',    'data': [        {            'embedding': [                -0.035126980394124985, -0.020624293014407158, -0.015343423001468182,                -0.03980357199907303, -0.02750781551003456, 0.02111034281551838,                -0.022069307044148445, -0.019442008808255196, -0.00955679826438427,                -0.013143060728907585, 0.029583381488919258, -0.004725852981209755,                -0.015198921784758568, -0.014069183729588985, 0.00897879246622324,                0.01521205808967352,                # ... (truncated for brevity)            ]        }    ]}Custom Callback APIs [Async]​infoThis is an Enterprise only feature Get Started with Enterprise hereUse this if you:Want to use custom callbacks written in a non Python programming languageWant your callbacks to run on a different microserviceStep 1. Create your generic logging API endpoint​Set up a generic API endpoint that can receive data in JSON format. The data will be included within a "data" field. Your server should support the following Request format:curl --location https://your-domain.com/log-event \     --request POST \     --header "Content-Type: application/json" \     --data '{       "data": {         "id": "chatcmpl-8sgE89cEQ4q9biRtxMvDfQU1O82PT",         "call_type": "acompletion",         "cache_hit": "None",         "startTime": "2024-02-15 16:18:44.336280",         "endTime": "2024-02-15 16:18:45.045539",         "model": "gpt-3.5-turbo",         "user": "ishaan-2",         "modelParameters": "{'temperature': 0.7, 'max_tokens': 10, 'user': 'ishaan-2', 'extra_body': {}}",         "messages": "[{'role': 'user', 'content': 'This is a test'}]",         "response": "ModelResponse(id='chatcmpl-8sgE89cEQ4q9biRtxMvDfQU1O82PT', choices=[Choices(finish_reason='length', index=0, message=Message(content='Great! How can I assist you with this test', role='assistant'))], created=1708042724, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=10, prompt_tokens=11, total_tokens=21))",         "usage": "Usage(completion_tokens=10, prompt_tokens=11, total_tokens=21)",         "metadata": "{}",         "cost": "3.65e-05"       }     }'Reference FastAPI Python ServerHere's a reference FastAPI Server that is compatible with LiteLLM Proxy:# this is an example endpoint to receive data from litellmfrom fastapi import FastAPI, HTTPException, Requestapp = FastAPI()@app.post("/log-event")async def log_event(request: Request):    try:        print("Received /log-event request")        # Assuming the incoming request has JSON data        data = await request.json()        print("Received request data:")        print(data)        # Your additional logic can go here        # For now, just printing the received data        return {"message": "Request received successfully"}    except Exception as e:        print(f"Error processing request: {str(e)}")        import traceback        traceback.print_exc()        raise HTTPException(status_code=500, detail="Internal Server Error")if __name__ == "__main__":    import uvicorn    uvicorn.run(app, host="127.0.0.1", port=4000)Step 2. Set your GENERIC_LOGGER_ENDPOINT to the endpoint + route we should send callback logs to​os.environ["GENERIC_LOGGER_ENDPOINT"] = "http://localhost:4000/log-event"Step 3. Create a config.yaml file and set litellm_settings: success_callback = ["generic"]​Example litellm proxy config.yamlmodel_list: - model_name: gpt-3.5-turbo    litellm_params:      model: gpt-3.5-turbolitellm_settings:  success_callback: ["generic"]Start the LiteLLM Proxy and make a test request to verify the logs reached your callback API Logging Proxy Cost + Usage - OpenMeter​Bill customers according to their LLM API usage with OpenMeterRequired Env Variables# from https://openmeter.cloudexport OPENMETER_API_ENDPOINT="" # defaults to https://openmeter.cloudexport OPENMETER_API_KEY=""Quick Start​Add to Config.yamlmodel_list:- litellm_params:    api_base: https://openai-function-calling-workers.tasslexyz.workers.dev/    api_key: my-fake-key    model: openai/my-fake-model  model_name: fake-openai-endpointlitellm_settings:  success_callback: ["openmeter"] # 👈 KEY CHANGEStart Proxylitellm --config /path/to/config.yamlTest it! curl --location 'http://0.0.0.0:4000/chat/completions' \--header 'Content-Type: application/json' \--data ' {      "model": "fake-openai-endpoint",      "messages": [        {          "role": "user",          "content": "what llm are you"        }      ],    }'Logging Proxy Input/Output - DataDog​We will use the --config to set litellm.success_callback = ["datadog"] this will log all successfull LLM calls to DataDogStep 1: Create a config.yaml file and set litellm_settings: success_callbackmodel_list: - model_name: gpt-3.5-turbo    litellm_params:      model: gpt-3.5-turbolitellm_settings:  success_callback: ["datadog"]Step 2: Set Required env variables for datadogDD_API_KEY="5f2d0f310***********" # your datadog API KeyDD_SITE="us5.datadoghq.com"       # your datadog base urlStep 3: Start the proxy, make a test requestStart proxylitellm --config config.yaml --debugTest Requestcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --data '{    "model": "gpt-3.5-turbo",    "messages": [        {        "role": "user",        "content": "what llm are you"        }    ],    "metadata": {        "your-custom-metadata": "custom-field",    }}'Expected output on DatadogLogging Proxy Input/Output - s3 Buckets​We will use the --config to set litellm.success_callback = ["s3"] This will log all successfull LLM calls to s3 BucketStep 1 Set AWS Credentials in .envAWS_ACCESS_KEY_ID = ""AWS_SECRET_ACCESS_KEY = ""AWS_REGION_NAME = ""Step 2: Create a config.yaml file and set litellm_settings: success_callbackmodel_list: - model_name: gpt-3.5-turbo    litellm_params:      model: gpt-3.5-turbolitellm_settings:  success_callback: ["s3"]  s3_callback_params:    s3_bucket_name: logs-bucket-litellm   # AWS Bucket Name for S3    s3_region_name: us-west-2              # AWS Region Name for S3    s3_aws_access_key_id: os.environ/AWS_ACCESS_KEY_ID  # us os.environ/<variable name> to pass environment variables. This is AWS Access Key ID for S3    s3_aws_secret_access_key: os.environ/AWS_SECRET_ACCESS_KEY  # AWS Secret Access Key for S3    s3_endpoint_url: https://s3.amazonaws.com  # [OPTIONAL] S3 endpoint URL, if you want to use Backblaze/cloudflare s3 bucketsStep 3: Start the proxy, make a test requestStart proxylitellm --config config.yaml --debugTest Requestcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --data ' {    "model": "Azure OpenAI GPT-4 East",    "messages": [        {        "role": "user",        "content": "what llm are you"        }    ]    }'Your logs should be available on the specified s3 BucketLogging Proxy Input/Output - DynamoDB​We will use the --config to set litellm.success_callback = ["dynamodb"] litellm.dynamodb_table_name = "your-table-name"This will log all successfull LLM calls to DynamoDBStep 1 Set AWS Credentials in .envAWS_ACCESS_KEY_ID = ""AWS_SECRET_ACCESS_KEY = ""AWS_REGION_NAME = ""Step 2: Create a config.yaml file and set litellm_settings: success_callbackmodel_list: - model_name: gpt-3.5-turbo    litellm_params:      model: gpt-3.5-turbolitellm_settings:  success_callback: ["dynamodb"]  dynamodb_table_name: your-table-nameStep 3: Start the proxy, make a test requestStart proxylitellm --config config.yaml --debugTest Requestcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --data ' {    "model": "Azure OpenAI GPT-4 East",    "messages": [        {        "role": "user",        "content": "what llm are you"        }    ]    }'Your logs should be available on DynamoDBData Logged to DynamoDB /chat/completions​{  "id": {    "S": "chatcmpl-8W15J4480a3fAQ1yQaMgtsKJAicen"  },  "call_type": {    "S": "acompletion"  },  "endTime": {    "S": "2023-12-15 17:25:58.424118"  },  "messages": {    "S": "[{'role': 'user', 'content': 'This is a test'}]"  },  "metadata": {    "S": "{}"  },  "model": {    "S": "gpt-3.5-turbo"  },  "modelParameters": {    "S": "{'temperature': 0.7, 'max_tokens': 100, 'user': 'ishaan-2'}"  },  "response": {    "S": "ModelResponse(id='chatcmpl-8W15J4480a3fAQ1yQaMgtsKJAicen', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Great! What can I assist you with?', role='assistant'))], created=1702641357, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=Usage(completion_tokens=9, prompt_tokens=11, total_tokens=20))"  },  "startTime": {    "S": "2023-12-15 17:25:56.047035"  },  "usage": {    "S": "Usage(completion_tokens=9, prompt_tokens=11, total_tokens=20)"  },  "user": {    "S": "ishaan-2"  }}Data logged to DynamoDB /embeddings​{  "id": {    "S": "4dec8d4d-4817-472d-9fc6-c7a6153eb2ca"  },  "call_type": {    "S": "aembedding"  },  "endTime": {    "S": "2023-12-15 17:25:59.890261"  },  "messages": {    "S": "['hi']"  },  "metadata": {    "S": "{}"  },  "model": {    "S": "text-embedding-ada-002"  },  "modelParameters": {    "S": "{'user': 'ishaan-2'}"  },  "response": {    "S": "EmbeddingResponse(model='text-embedding-ada-002-v2', data=[{'embedding': [-0.03503197431564331, -0.020601635798811913, -0.015375726856291294,  }}Logging Proxy Input/Output - Sentry​If api calls fail (llm/database) you can log those to Sentry: Step 1 Install Sentrypip install --upgrade sentry-sdkStep 2: Save your Sentry_DSN and add litellm_settings: failure_callbackexport SENTRY_DSN="your-sentry-dsn"model_list: - model_name: gpt-3.5-turbo    litellm_params:      model: gpt-3.5-turbolitellm_settings:  # other settings  failure_callback: ["sentry"]general_settings:   database_url: "my-bad-url" # set a fake url to trigger a sentry exceptionStep 3: Start the proxy, make a test requestStart proxylitellm --config config.yaml --debugTest Requestlitellm --testLogging Proxy Input/Output Athina​Athina allows you to log LLM Input/Output for monitoring, analytics, and observability.We will use the --config to set litellm.success_callback = ["athina"] this will log all successfull LLM calls to athinaStep 1 Set Athina API keyATHINA_API_KEY = "your-athina-api-key"Step 2: Create a config.yaml file and set litellm_settings: success_callbackmodel_list:  - model_name: gpt-3.5-turbo    litellm_params:      model: gpt-3.5-turbolitellm_settings:  success_callback: ["athina"]Step 3: Start the proxy, make a test requestStart proxylitellm --config config.yaml --debugTest Requestcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --data ' {    "model": "gpt-3.5-turbo",    "messages": [        {        "role": "user",        "content": "which llm are you"        }    ]    }'(BETA) Moderation with Azure Content Safety​Azure Content-Safety is a Microsoft Azure service that provides content moderation APIs to detect potential offensive, harmful, or risky content in text.We will use the --config to set litellm.success_callback = ["azure_content_safety"] this will moderate all LLM calls using Azure Content Safety.Step 0 Deploy Azure Content SafetyDeploy an Azure Content-Safety instance from the Azure Portal and get the endpoint and key.Step 1 Set Athina API keyAZURE_CONTENT_SAFETY_KEY = "<your-azure-content-safety-key>"Step 2: Create a config.yaml file and set litellm_settings: success_callbackmodel_list:  - model_name: gpt-3.5-turbo    litellm_params:      model: gpt-3.5-turbolitellm_settings:  callbacks: ["azure_content_safety"]  azure_content_safety_params:    endpoint: "<your-azure-content-safety-endpoint>"    key: "os.environ/AZURE_CONTENT_SAFETY_KEY"Step 3: Start the proxy, make a test requestStart proxylitellm --config config.yaml --debugTest Requestcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --data ' {        "model": "gpt-3.5-turbo",        "messages": [            {                "role": "user",                "content": "Hi, how are you?"            }        ]    }'An HTTP 400 error will be returned if the content is detected with a value greater than the threshold set in the config.yaml.
The details of the response will describe :The source : input text or llm generated textThe category : the category of the content that triggered the moderationThe severity : the severity from 0 to 10Step 4: Customizing Azure Content Safety ThresholdsYou can customize the thresholds for each category by setting the thresholds in the config.yamlmodel_list:  - model_name: gpt-3.5-turbo    litellm_params:      model: gpt-3.5-turbolitellm_settings:  callbacks: ["azure_content_safety"]  azure_content_safety_params:    endpoint: "<your-azure-content-safety-endpoint>"    key: "os.environ/AZURE_CONTENT_SAFETY_KEY"    thresholds:      Hate: 6      SelfHarm: 8      Sexual: 6      Violence: 4infothresholds are not required by default, but you can tune the values to your needs.
Default values is 4 for all categoriesPrevious🚨 Alerting / WebhooksNextTrack Token UsageLogging Proxy Input/Output - LangfuseLogging Metadata to LangfuseTeam based Logging to LangfuseRedacting Messages, Response Content from Langfuse Logging🔧 Debugging - Viewing RAW CURL sent from LiteLLM to providerLogging Proxy Input/Output in OpenTelemetry formatCustom Callback Class AsyncLogging responses from proxyCustom Callback APIs AsyncLogging Proxy Cost + Usage - OpenMeterQuick StartLogging Proxy Input/Output - DataDogLogging Proxy Input/Output - s3 BucketsLogging Proxy Input/Output - DynamoDBLogging Proxy Input/Output - SentryLogging Proxy Input/Output Athina(BETA) Moderation with Azure Content Safety

💥 OpenAI Proxy Server💰 Budgets, Rate LimitsOn this page💰 Budgets, Rate LimitsRequirements: Need to a postgres database (e.g. Supabase, Neon, etc) See SetupSet Budgets​You can set budgets at 3 levels: For the proxy For an internal user For a customer (end-user)For a keyFor a key (model specific budgets)For ProxyFor TeamFor Team MembersFor CustomersFor KeyFor Internal User (Global)For Key (model specific)Apply a budget across all calls on the proxyStep 1. Modify config.yamlgeneral_settings:  master_key: sk-1234litellm_settings:  # other litellm settings  max_budget: 0 # (float) sets max budget as $0 USD  budget_duration: 30d # (str) frequency of reset - You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d").Step 2. Start proxylitellm /path/to/config.yamlStep 3. Send test callcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Autherization: Bearer sk-1234' \    --header 'Content-Type: application/json' \    --data '{    "model": "gpt-3.5-turbo",    "messages": [        {        "role": "user",        "content": "what llm are you"        }    ],}'You can: - Add budgets to TeamsAdd budgets to users​curl --location 'http://localhost:4000/team/new' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{  "team_alias": "my-new-team_4",  "members_with_roles": [{"role": "admin", "user_id": "5c4a0aa3-a1e1-43dc-bd87-3c2da8382a3a"}],  "rpm_limit": 99}' See SwaggerSample Response{    "team_alias": "my-new-team_4",    "team_id": "13e83b19-f851-43fe-8e93-f96e21033100",    "admins": [],    "members": [],    "members_with_roles": [        {            "role": "admin",            "user_id": "5c4a0aa3-a1e1-43dc-bd87-3c2da8382a3a"        }    ],    "metadata": {},    "tpm_limit": null,    "rpm_limit": 99,    "max_budget": null,    "models": [],    "spend": 0.0,    "max_parallel_requests": null,    "budget_duration": null,    "budget_reset_at": null}Use this when you want to budget a users spend within a Team Step 1. Create User​Create a user with user_id=ishaancurl --location 'http://0.0.0.0:4000/user/new' \    --header 'Authorization: Bearer sk-1234' \    --header 'Content-Type: application/json' \    --data '{        "user_id": "ishaan"}'Step 2. Add User to an existing Team - set max_budget_in_team​Set max_budget_in_team when adding a User to a team. We use the same user_id we set in Step 1curl -X POST 'http://0.0.0.0:4000/team/member_add' \-H 'Authorization: Bearer sk-1234' \-H 'Content-Type: application/json' \-d '{"team_id": "e8d1460f-846c-45d7-9b43-55f3cc52ac32", "max_budget_in_team": 0.000000000001, "member": {"role": "user", "user_id": "ishaan"}}'Step 3. Create a Key for Team member from Step 1​Set user_id=ishaan from step 1curl --location 'http://0.0.0.0:4000/key/generate' \    --header 'Authorization: Bearer sk-1234' \    --header 'Content-Type: application/json' \    --data '{        "user_id": "ishaan",        "team_id": "e8d1460f-846c-45d7-9b43-55f3cc52ac32"}'Response from /key/generateWe use the key from this response in Step 4{"key":"sk-RV-l2BJEZ_LYNChSx2EueQ", "models":[],"spend":0.0,"max_budget":null,"user_id":"ishaan","team_id":"e8d1460f-846c-45d7-9b43-55f3cc52ac32","max_parallel_requests":null,"metadata":{},"tpm_limit":null,"rpm_limit":null,"budget_duration":null,"allowed_cache_controls":[],"soft_budget":null,"key_alias":null,"duration":null,"aliases":{},"config":{},"permissions":{},"model_max_budget":{},"key_name":null,"expires":null,"token_id":null}% Step 4. Make /chat/completions requests for Team member​Use the key from step 3 for this request. After 2-3 requests expect to see The following error ExceededBudget: Crossed spend within team curl --location 'http://localhost:4000/chat/completions' \    --header 'Authorization: Bearer sk-RV-l2BJEZ_LYNChSx2EueQ' \    --header 'Content-Type: application/json' \    --data '{    "model": "llama3",    "messages": [        {        "role": "user",        "content": "tes4"        }    ]}'Use this to budget user passed to /chat/completions, without needing to create a key for every userStep 1. Modify config.yaml
Define litellm.max_end_user_budgetgeneral_settings:  master_key: sk-1234litellm_settings:  max_end_user_budget: 0.0001 # budget for 'user' passed to /chat/completionsMake a /chat/completions call, pass 'user' - First call Works curl --location 'http://0.0.0.0:4000/chat/completions' \        --header 'Content-Type: application/json' \        --header 'Authorization: Bearer sk-zi5onDRdHGD24v0Zdn7VBA' \        --data ' {        "model": "azure-gpt-3.5",        "user": "ishaan3",        "messages": [            {            "role": "user",            "content": "what time is it"            }        ]        }'Make a /chat/completions call, pass 'user' - Call Fails, since 'ishaan3' over budgetcurl --location 'http://0.0.0.0:4000/chat/completions' \        --header 'Content-Type: application/json' \        --header 'Authorization: Bearer sk-zi5onDRdHGD24v0Zdn7VBA' \        --data ' {        "model": "azure-gpt-3.5",        "user": "ishaan3",        "messages": [            {            "role": "user",            "content": "what time is it"            }        ]        }'Error{"error":{"message":"Budget has been exceeded: User ishaan3 has exceeded their budget. Current spend: 0.0008869999999999999; Max Budget: 0.0001","type":"auth_error","param":"None","code":401}}%                Apply a budget on a key.You can:Add budgets to keys JumpAdd budget durations, to reset spend JumpExpected BehaviourCosts Per key get auto-populated in LiteLLM_VerificationToken TableAfter the key crosses it's max_budget, requests failIf duration set, spend is reset at the end of the durationBy default the max_budget is set to null and is not checked for keysAdd budgets to keys​curl 'http://0.0.0.0:4000/key/generate' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{  "team_id": "core-infra", # [OPTIONAL]  "max_budget": 10,}'Example Request to /chat/completions when key has crossed budgetcurl --location 'http://0.0.0.0:4000/chat/completions' \  --header 'Content-Type: application/json' \  --header 'Authorization: Bearer <generated-key>' \  --data ' {  "model": "azure-gpt-3.5",  "user": "e09b4da8-ed80-4b05-ac93-e16d9eb56fca",  "messages": [      {      "role": "user",      "content": "respond in 50 lines"      }  ],}'Expected Response from /chat/completions when key has crossed budget{  "detail":"Authentication Error, ExceededTokenBudget: Current spend for token: 7.2e-05; Max Budget for Token: 2e-07"}   Add budget duration to keys​budget_duration: Budget is reset at the end of specified duration. If not set, budget is never reset. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d").curl 'http://0.0.0.0:4000/key/generate' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{  "team_id": "core-infra", # [OPTIONAL]  "max_budget": 10,  "budget_duration": 10s,}'Apply a budget across all calls an internal user (key owner) can make on the proxy. infoFor most use-cases, we recommend setting team-member budgetsLiteLLM exposes a /user/new endpoint to create budgets for this.You can:Add budgets to users JumpAdd budget durations, to reset spend JumpBy default the max_budget is set to null and is not checked for keysAdd budgets to users​curl --location 'http://localhost:4000/user/new' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{"models": ["azure-models"], "max_budget": 0, "user_id": "krrish3@berri.ai"}' See SwaggerSample Response{    "key": "sk-YF2OxDbrgd1y2KgwxmEA2w",    "expires": "2023-12-22T09:53:13.861000Z",    "user_id": "krrish3@berri.ai",    "max_budget": 0.0}Add budget duration to users​budget_duration: Budget is reset at the end of specified duration. If not set, budget is never reset. You can set duration as seconds ("30s"), minutes ("30m"), hours ("30h"), days ("30d").curl 'http://0.0.0.0:4000/user/new' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{  "team_id": "core-infra", # [OPTIONAL]  "max_budget": 10,  "budget_duration": 10s,}'Create new keys for existing user​Now you can just call /key/generate with that user_id (i.e. krrish3@berri.ai) and:Budget Check: krrish3@berri.ai's budget (i.e. $10) will be checked for this keySpend Tracking: spend for this key will update krrish3@berri.ai's spend as wellcurl --location 'http://0.0.0.0:4000/key/generate' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data '{"models": ["azure-models"], "user_id": "krrish3@berri.ai"}'Apply model specific budgets on a key.Expected Behaviourmodel_spend gets auto-populated in LiteLLM_VerificationToken TableAfter the key crosses the budget set for the model in model_max_budget, calls failBy default the model_max_budget is set to {} and is not checked for keysinfoLiteLLM will track the cost/budgets for the model passed to LLM endpoints (/chat/completions, /embeddings)Add model specific budgets to keys​curl 'http://0.0.0.0:4000/key/generate' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{  model_max_budget={"gpt4": 0.5, "gpt-5": 0.01}}'Set Rate Limits​You can set: tpm limits (tokens per minute)rpm limits (requests per minute)max parallel requestsPer Internal UserPer KeyFor customersUse /user/new, to persist rate limits across multiple keys.curl --location 'http://0.0.0.0:4000/user/new' \--header 'Authorization: Bearer sk-1234' \--header 'Content-Type: application/json' \--data '{"user_id": "krrish@berri.ai", "max_parallel_requests": 10, "tpm_limit": 20, "rpm_limit": 4}' See SwaggerExpected Response{    "key": "sk-sA7VDkyhlQ7m8Gt77Mbt3Q",    "expires": "2024-01-19T01:21:12.816168",    "user_id": "krrish@berri.ai",}Use /key/generate, if you want them for just that key.curl --location 'http://0.0.0.0:4000/key/generate' \--header 'Authorization: Bearer sk-1234' \--header 'Content-Type: application/json' \--data '{"max_parallel_requests": 10, "tpm_limit": 20, "rpm_limit": 4}' Expected Response{    "key": "sk-ulGNRXWtv7M0lFnnsQk0wQ",    "expires": "2024-01-18T20:48:44.297973",    "user_id": "78c2c8fc-c233-43b9-b0c3-eb931da27b84"  // 👈 auto-generated}infoYou can also create a budget id for a customer on the UI, under the 'Rate Limits' tab.Use this to set rate limits for user passed to /chat/completions, without needing to create a key for every userStep 1. Create Budget​Set a tpm_limit on the budget (You can also pass rpm_limit if needed)curl --location 'http://0.0.0.0:4000/budget/new' \--header 'Authorization: Bearer sk-1234' \--header 'Content-Type: application/json' \--data '{    "budget_id" : "free-tier",    "tpm_limit": 5}'Step 2. Create Customer with Budget​We use budget_id="free-tier" from Step 1 when creating this new customerscurl --location 'http://0.0.0.0:4000/customer/new' \--header 'Authorization: Bearer sk-1234' \--header 'Content-Type: application/json' \--data '{    "user_id" : "palantir",    "budget_id": "free-tier"}'Step 3. Pass user_id id in /chat/completions requests​Pass the user_id from Step 2 as user="palantir" curl --location 'http://localhost:4000/chat/completions' \    --header 'Authorization: Bearer sk-1234' \    --header 'Content-Type: application/json' \    --data '{    "model": "llama3",    "user": "palantir",    "messages": [        {        "role": "user",        "content": "gm"        }    ]}'Grant Access to new model​Use model access groups to give users access to select models, and add new ones to it over time (e.g. mistral, llama-2, etc.). Difference between doing this with /key/generate vs. /user/new? If you do it on /user/new it'll persist across multiple keys generated for that user.Step 1. Assign model, access group in config.yamlmodel_list:  - model_name: text-embedding-ada-002    litellm_params:      model: azure/azure-embedding-model      api_base: "os.environ/AZURE_API_BASE"      api_key: "os.environ/AZURE_API_KEY"      api_version: "2023-07-01-preview"    model_info:      access_groups: ["beta-models"] # 👈 Model Access GroupStep 2. Create key with access groupcurl --location 'http://localhost:4000/user/new' \-H 'Authorization: Bearer <your-master-key>' \-H 'Content-Type: application/json' \-d '{"models": ["beta-models"], # 👈 Model Access Group            "max_budget": 0}'Create new keys for existing internal user​Just include user_id in the /key/generate request.curl --location 'http://0.0.0.0:4000/key/generate' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data '{"models": ["azure-models"], "user_id": "krrish@berri.ai"}'Previous💸 Spend TrackingNext🙋‍♂️ CustomersSet BudgetsSet Rate LimitsGrant Access to new modelCreate new keys for existing internal user

Logging & ObservabilityCustom CallbacksOn this pageCustom CallbacksinfoFor PROXY Go HereCallback Class​You can create a custom callback class to precisely log events as they occur in litellm. import litellmfrom litellm.integrations.custom_logger import CustomLoggerfrom litellm import completion, acompletionclass MyCustomHandler(CustomLogger):    def log_pre_api_call(self, model, messages, kwargs):         print(f"Pre-API Call")        def log_post_api_call(self, kwargs, response_obj, start_time, end_time):         print(f"Post-API Call")        def log_stream_event(self, kwargs, response_obj, start_time, end_time):        print(f"On Stream")            def log_success_event(self, kwargs, response_obj, start_time, end_time):         print(f"On Success")    def log_failure_event(self, kwargs, response_obj, start_time, end_time):         print(f"On Failure")        #### ASYNC #### - for acompletion/aembeddings        async def async_log_stream_event(self, kwargs, response_obj, start_time, end_time):        print(f"On Async Streaming")    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):        print(f"On Async Success")    async def async_log_failure_event(self, kwargs, response_obj, start_time, end_time):        print(f"On Async Success")customHandler = MyCustomHandler()litellm.callbacks = [customHandler]## sync response = completion(model="gpt-3.5-turbo", messages=[{ "role": "user", "content": "Hi 👋 - i'm openai"}],                              stream=True)for chunk in response:     continue## asyncimport asyncio def async completion():    response = await acompletion(model="gpt-3.5-turbo", messages=[{ "role": "user", "content": "Hi 👋 - i'm openai"}],                              stream=True)    async for chunk in response:         continueasyncio.run(completion())Callback Functions​If you just want to log on a specific event (e.g. on input) - you can use callback functions. You can set custom callbacks to trigger for:litellm.input_callback   - Track inputs/transformed inputs before making the LLM API calllitellm.success_callback - Track inputs/outputs after making LLM API calllitellm.failure_callback - Track inputs/outputs + exceptions for litellm callsDefining a Custom Callback Function​Create a custom callback function that takes specific arguments:def custom_callback(    kwargs,                 # kwargs to completion    completion_response,    # response from completion    start_time, end_time    # start/end time):    # Your custom code here    print("LITELLM: in custom callback function")    print("kwargs", kwargs)    print("completion_response", completion_response)    print("start_time", start_time)    print("end_time", end_time)Setting the custom callback function​import litellmlitellm.success_callback = [custom_callback]Using Your Custom Callback Function​import litellmfrom litellm import completion# Assign the custom callback functionlitellm.success_callback = [custom_callback]response = completion(    model="gpt-3.5-turbo",    messages=[        {            "role": "user",            "content": "Hi 👋 - i'm openai"        }    ])print(response)Async Callback Functions​We recommend using the Custom Logger class for async.from litellm.integrations.custom_logger import CustomLoggerfrom litellm import acompletion class MyCustomHandler(CustomLogger):    #### ASYNC ####         async def async_log_stream_event(self, kwargs, response_obj, start_time, end_time):        print(f"On Async Streaming")    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):        print(f"On Async Success")    async def async_log_failure_event(self, kwargs, response_obj, start_time, end_time):        print(f"On Async Failure")import asyncio customHandler = MyCustomHandler()litellm.callbacks = [customHandler]def async completion():    response = await acompletion(model="gpt-3.5-turbo", messages=[{ "role": "user", "content": "Hi 👋 - i'm openai"}],                              stream=True)    async for chunk in response:         continueasyncio.run(completion())FunctionsIf you just want to pass in an async function for logging. LiteLLM currently supports just async success callback functions for async completion/embedding calls. import asyncio, litellm async def async_test_logging_fn(kwargs, completion_obj, start_time, end_time):    print(f"On Async Success!")async def test_chat_openai():    try:        # litellm.set_verbose = True        litellm.success_callback = [async_test_logging_fn]        response = await litellm.acompletion(model="gpt-3.5-turbo",                              messages=[{                                  "role": "user",                                  "content": "Hi 👋 - i'm openai"                              }],                              stream=True)        async for chunk in response:             continue    except Exception as e:        print(e)        pytest.fail(f"An error occurred - {str(e)}")asyncio.run(test_chat_openai())infoWe're actively trying to expand this to other event types. Tell us if you need this!What's in kwargs?​Notice we pass in a kwargs argument to custom callback. def custom_callback(    kwargs,                 # kwargs to completion    completion_response,    # response from completion    start_time, end_time    # start/end time):    # Your custom code here    print("LITELLM: in custom callback function")    print("kwargs", kwargs)    print("completion_response", completion_response)    print("start_time", start_time)    print("end_time", end_time)This is a dictionary containing all the model-call details (the params we receive, the values we send to the http endpoint, the response we receive, stacktrace in case of errors, etc.). This is all logged in the model_call_details via our Logger.Here's exactly what you can expect in the kwargs dictionary:### DEFAULT PARAMS ### "model": self.model,"messages": self.messages,"optional_params": self.optional_params, # model-specific params passed in"litellm_params": self.litellm_params, # litellm-specific params passed in (e.g. metadata passed to completion call)"start_time": self.start_time, # datetime object of when call was started### PRE-API CALL PARAMS ### (check via kwargs["log_event_type"]="pre_api_call")"input" = input # the exact prompt sent to the LLM API"api_key" = api_key # the api key used for that LLM API "additional_args" = additional_args # any additional details for that API call (e.g. contains optional params sent)### POST-API CALL PARAMS ### (check via kwargs["log_event_type"]="post_api_call")"original_response" = original_response # the original http response received (saved via response.text)### ON-SUCCESS PARAMS ### (check via kwargs["log_event_type"]="successful_api_call")"complete_streaming_response" = complete_streaming_response # the complete streamed response (only set if `completion(..stream=True)`)"end_time" = end_time # datetime object of when call was completed### ON-FAILURE PARAMS ### (check via kwargs["log_event_type"]="failed_api_call")"exception" = exception # the Exception raised"traceback_exception" = traceback_exception # the traceback generated via `traceback.format_exc()`"end_time" = end_time # datetime object of when call was completedCache hits​Cache hits are logged in success events as kwarg["cache_hit"]. Here's an example of accessing it: import litellmfrom litellm.integrations.custom_logger import CustomLoggerfrom litellm import completion, acompletion, Cacheclass MyCustomHandler(CustomLogger):  async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):       print(f"On Success")      print(f"Value of Cache hit: {kwargs['cache_hit']"})async def test_async_completion_azure_caching():  customHandler_caching = MyCustomHandler()  litellm.cache = Cache(type="redis", host=os.environ['REDIS_HOST'], port=os.environ['REDIS_PORT'], password=os.environ['REDIS_PASSWORD'])  litellm.callbacks = [customHandler_caching]  unique_time = time.time()  response1 = await litellm.acompletion(model="azure/chatgpt-v-2",                          messages=[{                              "role": "user",                              "content": f"Hi 👋 - i'm async azure {unique_time}"                          }],                          caching=True)  await asyncio.sleep(1)  print(f"customHandler_caching.states pre-cache hit: {customHandler_caching.states}")  response2 = await litellm.acompletion(model="azure/chatgpt-v-2",                          messages=[{                              "role": "user",                              "content": f"Hi 👋 - i'm async azure {unique_time}"                          }],                          caching=True)  await asyncio.sleep(1) # success callbacks are done in parallel  print(f"customHandler_caching.states post-cache hit: {customHandler_caching.states}")  assert len(customHandler_caching.errors) == 0  assert len(customHandler_caching.states) == 4 # pre, post, success, successGet complete streaming response​LiteLLM will pass you the complete streaming response in the final streaming chunk as part of the kwargs for your custom callback function.# litellm.set_verbose = False        def custom_callback(            kwargs,                 # kwargs to completion            completion_response,    # response from completion            start_time, end_time    # start/end time        ):            # print(f"streaming response: {completion_response}")            if "complete_streaming_response" in kwargs:                 print(f"Complete Streaming Response: {kwargs['complete_streaming_response']}")                # Assign the custom callback function        litellm.success_callback = [custom_callback]        response = completion(model="claude-instant-1", messages=messages, stream=True)        for idx, chunk in enumerate(response):             passLog additional metadata​LiteLLM accepts a metadata dictionary in the completion call. You can pass additional metadata into your completion call via completion(..., metadata={"key": "value"}). Since this is a litellm-specific param, it's accessible via kwargs["litellm_params"]from litellm import completionimport os, litellm## set ENV variablesos.environ["OPENAI_API_KEY"] = "your-api-key"messages = [{ "content": "Hello, how are you?","role": "user"}]def custom_callback(    kwargs,                 # kwargs to completion    completion_response,    # response from completion    start_time, end_time    # start/end time):    print(kwargs["litellm_params"]["metadata"])    # Assign the custom callback functionlitellm.success_callback = [custom_callback]response = litellm.completion(model="gpt-3.5-turbo", messages=messages, metadata={"hello": "world"})Examples​Custom Callback to track costs for Streaming + Non-Streaming​By default, the response cost is accessible in the logging object via kwargs["response_cost"] on success (sync + async)# Step 1. Write your custom callback functiondef track_cost_callback(    kwargs,                 # kwargs to completion    completion_response,    # response from completion    start_time, end_time    # start/end time):    try:        response_cost = kwargs["response_cost"] # litellm calculates response cost for you        print("regular response_cost", response_cost)    except:        pass# Step 2. Assign the custom callback functionlitellm.success_callback = [track_cost_callback]# Step 3. Make litellm.completion callresponse = completion(    model="gpt-3.5-turbo",    messages=[        {            "role": "user",            "content": "Hi 👋 - i'm openai"        }    ])print(response)Custom Callback to log transformed Input to LLMs​def get_transformed_inputs(    kwargs,):    params_to_model = kwargs["additional_args"]["complete_input_dict"]    print("params to model", params_to_model)litellm.input_callback = [get_transformed_inputs]def test_chat_openai():    try:        response = completion(model="claude-2",                              messages=[{                                  "role": "user",                                  "content": "Hi 👋 - i'm openai"                              }])        print(response)    except Exception as e:        print(e)        passOutput​params to model {'model': 'claude-2', 'prompt': "\n\nHuman: Hi 👋 - i'm openai\n\nAssistant: ", 'max_tokens_to_sample': 256}Custom Callback to write to Mixpanel​import mixpanelimport litellmfrom litellm import completiondef custom_callback(    kwargs,                 # kwargs to completion    completion_response,    # response from completion    start_time, end_time    # start/end time):    # Your custom code here    mixpanel.track("LLM Response", {"llm_response": completion_response})# Assign the custom callback functionlitellm.success_callback = [custom_callback]response = completion(    model="gpt-3.5-turbo",    messages=[        {            "role": "user",            "content": "Hi 👋 - i'm openai"        }    ])print(response)PreviousCallbacksNextLangfuse - Logging LLM Input/OutputCallback ClassCallback FunctionsDefining a Custom Callback FunctionSetting the custom callback functionUsing Your Custom Callback FunctionAsync Callback FunctionsWhat's in kwargs?Cache hitsGet complete streaming responseLog additional metadataExamplesCustom Callback to track costs for Streaming + Non-StreamingCustom Callback to log transformed Input to LLMsCustom Callback to write to Mixpanel

Router - Load Balancing, FallbacksOn this pageRouter - Load Balancing, FallbacksLiteLLM manages:Load-balance across multiple deployments (e.g. Azure/OpenAI)Prioritizing important requests to ensure they don't fail (i.e. Queueing)Basic reliability logic - cooldowns, fallbacks, timeouts and retries (fixed + exponential backoff) across multiple deployments/providers.In production, litellm supports using Redis as a way to track cooldown server and usage (managing tpm/rpm limits).infoIf you want a server to load balance across different LLM APIs, use our OpenAI Proxy ServerLoad Balancing​(s/o @paulpierre and sweep proxy for their contributions to this implementation)
See CodeQuick Start​from litellm import Routermodel_list = [{ # list of model deployments     "model_name": "gpt-3.5-turbo", # model alias -> loadbalance between models with same `model_name`    "litellm_params": { # params for litellm completion/embedding call         "model": "azure/chatgpt-v-2", # actual model name        "api_key": os.getenv("AZURE_API_KEY"),        "api_version": os.getenv("AZURE_API_VERSION"),        "api_base": os.getenv("AZURE_API_BASE")    }}, {    "model_name": "gpt-3.5-turbo",     "litellm_params": { # params for litellm completion/embedding call         "model": "azure/chatgpt-functioncalling",         "api_key": os.getenv("AZURE_API_KEY"),        "api_version": os.getenv("AZURE_API_VERSION"),        "api_base": os.getenv("AZURE_API_BASE")    }}, {    "model_name": "gpt-3.5-turbo",     "litellm_params": { # params for litellm completion/embedding call         "model": "gpt-3.5-turbo",         "api_key": os.getenv("OPENAI_API_KEY"),    }}, {    "model_name": "gpt-4",     "litellm_params": { # params for litellm completion/embedding call         "model": "azure/gpt-4",         "api_key": os.getenv("AZURE_API_KEY"),        "api_base": os.getenv("AZURE_API_BASE"),        "api_version": os.getenv("AZURE_API_VERSION"),    }}, {    "model_name": "gpt-4",     "litellm_params": { # params for litellm completion/embedding call         "model": "gpt-4",         "api_key": os.getenv("OPENAI_API_KEY"),    }},]router = Router(model_list=model_list)# openai.ChatCompletion.create replacement# requests with model="gpt-3.5-turbo" will pick a deployment where model_name="gpt-3.5-turbo"response = await router.acompletion(model="gpt-3.5-turbo",                 messages=[{"role": "user", "content": "Hey, how's it going?"}])print(response)# openai.ChatCompletion.create replacement# requests with model="gpt-4" will pick a deployment where model_name="gpt-4"response = await router.acompletion(model="gpt-4",                 messages=[{"role": "user", "content": "Hey, how's it going?"}])print(response)Available Endpoints​router.completion() - chat completions endpoint to call 100+ LLMsrouter.acompletion() - async chat completion callsrouter.embeddings() - embedding endpoint for Azure, OpenAI, Huggingface endpointsrouter.aembeddings() - async embeddings callsrouter.text_completion() - completion calls in the old OpenAI /v1/completions endpoint formatrouter.atext_completion() - async text completion callsrouter.image_generation() - completion calls in OpenAI /v1/images/generations endpoint formatrouter.aimage_generation() - async image generation callsAdvanced - Routing Strategies​Routing Strategies - Weighted Pick, Rate Limit Aware, Least Busy, Latency Based, Cost Based​Router provides 4 strategies for routing your calls across multiple deployments: Rate-Limit Aware v2 (ASYNC)Latency-Based(Default) Weighted Pick (Async)Rate-Limit AwareLeast-BusyLowest Cost Routing (Async)🎉 NEW This is an async implementation of usage-based-routing.Filters out deployment if tpm/rpm limit exceeded - If you pass in the deployment's tpm/rpm limits.Routes to deployment with lowest TPM usage for that minute. In production, we use Redis to track usage (TPM/RPM) across multiple deployments. This implementation uses async redis calls (redis.incr and redis.mget).For Azure, your RPM = TPM/6. sdkproxyfrom litellm import Router model_list = [{ # list of model deployments     "model_name": "gpt-3.5-turbo", # model alias     "litellm_params": { # params for litellm completion/embedding call         "model": "azure/chatgpt-v-2", # actual model name        "api_key": os.getenv("AZURE_API_KEY"),        "api_version": os.getenv("AZURE_API_VERSION"),        "api_base": os.getenv("AZURE_API_BASE")    },     "tpm": 100000,    "rpm": 10000,}, {    "model_name": "gpt-3.5-turbo",     "litellm_params": { # params for litellm completion/embedding call         "model": "azure/chatgpt-functioncalling",         "api_key": os.getenv("AZURE_API_KEY"),        "api_version": os.getenv("AZURE_API_VERSION"),        "api_base": os.getenv("AZURE_API_BASE")    },    "tpm": 100000,    "rpm": 1000,}, {    "model_name": "gpt-3.5-turbo",     "litellm_params": { # params for litellm completion/embedding call         "model": "gpt-3.5-turbo",         "api_key": os.getenv("OPENAI_API_KEY"),    },    "tpm": 100000,    "rpm": 1000,}]router = Router(model_list=model_list,                 redis_host=os.environ["REDIS_HOST"],                 redis_password=os.environ["REDIS_PASSWORD"],                 redis_port=os.environ["REDIS_PORT"],                 routing_strategy="usage-based-routing-v2" # 👈 KEY CHANGE                enable_pre_call_check=True, # enables router rate limits for concurrent calls                )response = await router.acompletion(model="gpt-3.5-turbo",                 messages=[{"role": "user", "content": "Hey, how's it going?"}]print(response)1. Set strategy in configmodel_list:    - model_name: gpt-3.5-turbo # model alias       litellm_params: # params for litellm completion/embedding call         model: azure/chatgpt-v-2 # actual model name        api_key: os.environ/AZURE_API_KEY        api_version: os.environ/AZURE_API_VERSION        api_base: os.environ/AZURE_API_BASE      tpm: 100000      rpm: 10000    - model_name: gpt-3.5-turbo       litellm_params: # params for litellm completion/embedding call         model: gpt-3.5-turbo         api_key: os.getenv(OPENAI_API_KEY)      tpm: 100000      rpm: 1000router_settings:  routing_strategy: usage-based-routing-v2 # 👈 KEY CHANGE  redis_host: <your-redis-host>  redis_password: <your-redis-password>  redis_port: <your-redis-port>  enable_pre_call_check: truegeneral_settings:  master_key: sk-12342. Start proxylitellm --config /path/to/config.yaml3. Test it!curl --location 'http://localhost:4000/v1/chat/completions' \--header 'Content-Type: application/json' \--header 'Authorization: Bearer sk-1234' \--data '{    "model": "gpt-3.5-turbo",     "messages": [{"role": "user", "content": "Hey, how's it going?"}]}'Picks the deployment with the lowest response time.It caches, and updates the response times for deployments based on when a request was sent and received from a deployment.How to testfrom litellm import Router import asynciomodel_list = [{ ... }]# init routerrouter = Router(model_list=model_list,                routing_strategy="latency-based-routing",# 👈 set routing strategy                enable_pre_call_check=True, # enables router rate limits for concurrent calls                )## CALL 1+2tasks = []response = Nonefinal_response = Nonefor _ in range(2):    tasks.append(router.acompletion(model=model, messages=messages))response = await asyncio.gather(*tasks)if response is not None:    ## CALL 3     await asyncio.sleep(1)  # let the cache update happen    picked_deployment = router.lowestlatency_logger.get_available_deployments(        model_group=model, healthy_deployments=router.healthy_deployments    )    final_response = await router.acompletion(model=model, messages=messages)    print(f"min deployment id: {picked_deployment}")    print(f"model id: {final_response._hidden_params['model_id']}")    assert (        final_response._hidden_params["model_id"]        == picked_deployment["model_info"]["id"]    )Set Time Window​Set time window for how far back to consider when averaging latency for a deployment. In Routerrouter = Router(..., routing_strategy_args={"ttl": 10})In Proxyrouter_settings:    routing_strategy_args: {"ttl": 10}Set Lowest Latency Buffer​Set a buffer within which deployments are candidates for making calls to. E.g. if you have 5 deploymentshttps://litellm-prod-1.openai.azure.com/: 0.07shttps://litellm-prod-2.openai.azure.com/: 0.1shttps://litellm-prod-3.openai.azure.com/: 0.1shttps://litellm-prod-4.openai.azure.com/: 0.1shttps://litellm-prod-5.openai.azure.com/: 4.66sto prevent initially overloading prod-1, with all requests - we can set a buffer of 50%, to consider deployments prod-2, prod-3, prod-4. In Routerrouter = Router(..., routing_strategy_args={"lowest_latency_buffer": 0.5})In Proxyrouter_settings:    routing_strategy_args: {"lowest_latency_buffer": 0.5}Default Picks a deployment based on the provided Requests per minute (rpm) or Tokens per minute (tpm)If rpm or tpm is not provided, it randomly picks a deploymentfrom litellm import Router import asynciomodel_list = [{ # list of model deployments     "model_name": "gpt-3.5-turbo", # model alias     "litellm_params": { # params for litellm completion/embedding call         "model": "azure/chatgpt-v-2", # actual model name        "api_key": os.getenv("AZURE_API_KEY"),        "api_version": os.getenv("AZURE_API_VERSION"),        "api_base": os.getenv("AZURE_API_BASE"),        "rpm": 900,         # requests per minute for this API    }}, {    "model_name": "gpt-3.5-turbo",     "litellm_params": { # params for litellm completion/embedding call         "model": "azure/chatgpt-functioncalling",         "api_key": os.getenv("AZURE_API_KEY"),        "api_version": os.getenv("AZURE_API_VERSION"),        "api_base": os.getenv("AZURE_API_BASE"),        "rpm": 10,    }}, {    "model_name": "gpt-3.5-turbo",     "litellm_params": { # params for litellm completion/embedding call         "model": "gpt-3.5-turbo",         "api_key": os.getenv("OPENAI_API_KEY"),        "rpm": 10,    }}]# init routerrouter = Router(model_list=model_list, routing_strategy="simple-shuffle")async def router_acompletion():    response = await router.acompletion(        model="gpt-3.5-turbo",         messages=[{"role": "user", "content": "Hey, how's it going?"}]    )    print(response)    return responseasyncio.run(router_acompletion())This will route to the deployment with the lowest TPM usage for that minute. In production, we use Redis to track usage (TPM/RPM) across multiple deployments. If you pass in the deployment's tpm/rpm limits, this will also check against that, and filter out any who's limits would be exceeded. For Azure, your RPM = TPM/6. from litellm import Router model_list = [{ # list of model deployments     "model_name": "gpt-3.5-turbo", # model alias     "litellm_params": { # params for litellm completion/embedding call         "model": "azure/chatgpt-v-2", # actual model name        "api_key": os.getenv("AZURE_API_KEY"),        "api_version": os.getenv("AZURE_API_VERSION"),        "api_base": os.getenv("AZURE_API_BASE")    },     "tpm": 100000,    "rpm": 10000,}, {    "model_name": "gpt-3.5-turbo",     "litellm_params": { # params for litellm completion/embedding call         "model": "azure/chatgpt-functioncalling",         "api_key": os.getenv("AZURE_API_KEY"),        "api_version": os.getenv("AZURE_API_VERSION"),        "api_base": os.getenv("AZURE_API_BASE")    },    "tpm": 100000,    "rpm": 1000,}, {    "model_name": "gpt-3.5-turbo",     "litellm_params": { # params for litellm completion/embedding call         "model": "gpt-3.5-turbo",         "api_key": os.getenv("OPENAI_API_KEY"),    },    "tpm": 100000,    "rpm": 1000,}]router = Router(model_list=model_list,                 redis_host=os.environ["REDIS_HOST"],                 redis_password=os.environ["REDIS_PASSWORD"],                 redis_port=os.environ["REDIS_PORT"],                 routing_strategy="usage-based-routing"                enable_pre_call_check=True, # enables router rate limits for concurrent calls                )response = await router.acompletion(model="gpt-3.5-turbo",                 messages=[{"role": "user", "content": "Hey, how's it going?"}]print(response)Picks a deployment with the least number of ongoing calls, it's handling.How to testfrom litellm import Router import asynciomodel_list = [{ # list of model deployments     "model_name": "gpt-3.5-turbo", # model alias     "litellm_params": { # params for litellm completion/embedding call         "model": "azure/chatgpt-v-2", # actual model name        "api_key": os.getenv("AZURE_API_KEY"),        "api_version": os.getenv("AZURE_API_VERSION"),        "api_base": os.getenv("AZURE_API_BASE"),    }}, {    "model_name": "gpt-3.5-turbo",     "litellm_params": { # params for litellm completion/embedding call         "model": "azure/chatgpt-functioncalling",         "api_key": os.getenv("AZURE_API_KEY"),        "api_version": os.getenv("AZURE_API_VERSION"),        "api_base": os.getenv("AZURE_API_BASE"),    }}, {    "model_name": "gpt-3.5-turbo",     "litellm_params": { # params for litellm completion/embedding call         "model": "gpt-3.5-turbo",         "api_key": os.getenv("OPENAI_API_KEY"),    }}]# init routerrouter = Router(model_list=model_list, routing_strategy="least-busy")async def router_acompletion():    response = await router.acompletion(        model="gpt-3.5-turbo",         messages=[{"role": "user", "content": "Hey, how's it going?"}]    )    print(response)    return responseasyncio.run(router_acompletion())Picks a deployment based on the lowest costHow this works:Get all healthy deploymentsSelect all deployments that are under their provided rpm/tpm limitsFor each deployment check if litellm_param["model"] exists in litellm_model_cost_map if deployment does not exist in litellm_model_cost_map -> use deployment_cost= $1Select deployment with lowest costfrom litellm import Router import asynciomodel_list =  [    {        "model_name": "gpt-3.5-turbo",        "litellm_params": {"model": "gpt-4"},        "model_info": {"id": "openai-gpt-4"},    },    {        "model_name": "gpt-3.5-turbo",        "litellm_params": {"model": "groq/llama3-8b-8192"},        "model_info": {"id": "groq-llama"},    },]# init routerrouter = Router(model_list=model_list, routing_strategy="cost-based-routing")async def router_acompletion():    response = await router.acompletion(        model="gpt-3.5-turbo",         messages=[{"role": "user", "content": "Hey, how's it going?"}]    )    print(response)    print(response._hidden_params["model_id"]) # expect groq-llama, since groq/llama has lowest cost    return responseasyncio.run(router_acompletion())Using Custom Input/Output pricing​Set litellm_params["input_cost_per_token"] and litellm_params["output_cost_per_token"] for using custom pricing when routingmodel_list = [    {        "model_name": "gpt-3.5-turbo",        "litellm_params": {            "model": "azure/chatgpt-v-2",            "input_cost_per_token": 0.00003,            "output_cost_per_token": 0.00003,        },        "model_info": {"id": "chatgpt-v-experimental"},    },    {        "model_name": "gpt-3.5-turbo",        "litellm_params": {            "model": "azure/chatgpt-v-1",            "input_cost_per_token": 0.000000001,            "output_cost_per_token": 0.00000001,        },        "model_info": {"id": "chatgpt-v-1"},    },    {        "model_name": "gpt-3.5-turbo",        "litellm_params": {            "model": "azure/chatgpt-v-5",            "input_cost_per_token": 10,            "output_cost_per_token": 12,        },        "model_info": {"id": "chatgpt-v-5"},    },]# init routerrouter = Router(model_list=model_list, routing_strategy="cost-based-routing")async def router_acompletion():    response = await router.acompletion(        model="gpt-3.5-turbo",         messages=[{"role": "user", "content": "Hey, how's it going?"}]    )    print(response)    print(response._hidden_params["model_id"]) # expect chatgpt-v-1, since chatgpt-v-1 has lowest cost    return responseasyncio.run(router_acompletion())Basic Reliability​Max Parallel Requests (ASYNC)​Used in semaphore for async requests on router. Limit the max concurrent calls made to a deployment. Useful in high-traffic scenarios. If tpm/rpm is set, and no max parallel request limit given, we use the RPM or calculated RPM (tpm/1000/6) as the max parallel request limit. from litellm import Router model_list = [{    "model_name": "gpt-4",    "litellm_params": {        "model": "azure/gpt-4",        ...        "max_parallel_requests": 10 # 👈 SET PER DEPLOYMENT    }}]### OR ### router = Router(model_list=model_list, default_max_parallel_requests=20) # 👈 SET DEFAULT MAX PARALLEL REQUESTS # deployment max parallel requests > default max parallel requestsSee CodeTimeouts​The timeout set in router is for the entire length of the call, and is passed down to the completion() call level as well. Global Timeoutsfrom litellm import Router model_list = [{...}]router = Router(model_list=model_list,                 timeout=30) # raise timeout error if call takes > 30s print(response)Timeouts per modelfrom litellm import Router import asynciomodel_list = [{    "model_name": "gpt-3.5-turbo",    "litellm_params": {        "model": "azure/chatgpt-v-2",        "api_key": os.getenv("AZURE_API_KEY"),        "api_version": os.getenv("AZURE_API_VERSION"),        "api_base": os.getenv("AZURE_API_BASE"),        "timeout": 300 # sets a 5 minute timeout        "stream_timeout": 30 # sets a 30s timeout for streaming calls    }}]# init routerrouter = Router(model_list=model_list, routing_strategy="least-busy")async def router_acompletion():    response = await router.acompletion(        model="gpt-3.5-turbo",         messages=[{"role": "user", "content": "Hey, how's it going?"}]    )    print(response)    return responseasyncio.run(router_acompletion())Cooldowns​Set the limit for how many calls a model is allowed to fail in a minute, before being cooled down for a minute. from litellm import Routermodel_list = [{...}]router = Router(model_list=model_list,                 allowed_fails=1,      # cooldown model if it fails > 1 call in a minute.                 cooldown_time=100    # cooldown the deployment for 100 seconds if it num_fails > allowed_fails        )user_message = "Hello, whats the weather in San Francisco??"messages = [{"content": user_message, "role": "user"}]# normal call response = router.completion(model="gpt-3.5-turbo", messages=messages)print(f"response: {response}")Retries​For both async + sync functions, we support retrying failed requests. For RateLimitError we implement exponential backoffs For generic errors, we retry immediately Here's a quick look at how we can set num_retries = 3: from litellm import Routermodel_list = [{...}]router = Router(model_list=model_list,                  num_retries=3)user_message = "Hello, whats the weather in San Francisco??"messages = [{"content": user_message, "role": "user"}]# normal call response = router.completion(model="gpt-3.5-turbo", messages=messages)print(f"response: {response}")We also support setting minimum time to wait before retrying a failed request. This is via the retry_after param. from litellm import Routermodel_list = [{...}]router = Router(model_list=model_list,                  num_retries=3, retry_after=5) # waits min 5s before retrying requestuser_message = "Hello, whats the weather in San Francisco??"messages = [{"content": user_message, "role": "user"}]# normal call response = router.completion(model="gpt-3.5-turbo", messages=messages)print(f"response: {response}")[Advanced]: Custom Retries, Cooldowns based on Error Type​Use RetryPolicy if you want to set a num_retries based on the Exception receievedUse AllowedFailsPolicy to set a custom number of allowed_fails/minute before cooling down a deploymentExample:retry_policy = RetryPolicy(    ContentPolicyViolationErrorRetries=3,         # run 3 retries for ContentPolicyViolationErrors    AuthenticationErrorRetries=0,                 # run 0 retries for AuthenticationErrorRetries)allowed_fails_policy = AllowedFailsPolicy(    ContentPolicyViolationErrorAllowedFails=1000, # Allow 1000 ContentPolicyViolationError before cooling down a deployment    RateLimitErrorAllowedFails=100,               # Allow 100 RateLimitErrors before cooling down a deployment)Example Usagefrom litellm.router import RetryPolicy, AllowedFailsPolicyretry_policy = RetryPolicy(    ContentPolicyViolationErrorRetries=3,         # run 3 retries for ContentPolicyViolationErrors    AuthenticationErrorRetries=0,                 # run 0 retries for AuthenticationErrorRetries    BadRequestErrorRetries=1,    TimeoutErrorRetries=2,    RateLimitErrorRetries=3,)allowed_fails_policy = AllowedFailsPolicy(    ContentPolicyViolationErrorAllowedFails=1000, # Allow 1000 ContentPolicyViolationError before cooling down a deployment    RateLimitErrorAllowedFails=100,               # Allow 100 RateLimitErrors before cooling down a deployment)router = litellm.Router(    model_list=[        {            "model_name": "gpt-3.5-turbo",  # openai model name            "litellm_params": {  # params for litellm completion/embedding call                "model": "azure/chatgpt-v-2",                "api_key": os.getenv("AZURE_API_KEY"),                "api_version": os.getenv("AZURE_API_VERSION"),                "api_base": os.getenv("AZURE_API_BASE"),            },        },        {            "model_name": "bad-model",  # openai model name            "litellm_params": {  # params for litellm completion/embedding call                "model": "azure/chatgpt-v-2",                "api_key": "bad-key",                "api_version": os.getenv("AZURE_API_VERSION"),                "api_base": os.getenv("AZURE_API_BASE"),            },        },    ],    retry_policy=retry_policy,    allowed_fails_policy=allowed_fails_policy,)response = await router.acompletion(    model=model,    messages=messages,)Fallbacks​If a call fails after num_retries, fall back to another model group. If the error is a context window exceeded error, fall back to a larger model group (if given). Fallbacks are done in-order - ["gpt-3.5-turbo, "gpt-4", "gpt-4-32k"], will do 'gpt-3.5-turbo' first, then 'gpt-4', etc.You can also set 'default_fallbacks', in case a specific model group is misconfigured / bad.from litellm import Routermodel_list = [    { # list of model deployments         "model_name": "azure/gpt-3.5-turbo", # openai model name         "litellm_params": { # params for litellm completion/embedding call             "model": "azure/chatgpt-v-2",             "api_key": "bad-key",            "api_version": os.getenv("AZURE_API_VERSION"),            "api_base": os.getenv("AZURE_API_BASE")        },        "tpm": 240000,        "rpm": 1800    },     { # list of model deployments         "model_name": "azure/gpt-3.5-turbo-context-fallback", # openai model name         "litellm_params": { # params for litellm completion/embedding call             "model": "azure/chatgpt-v-2",             "api_key": "bad-key",            "api_version": os.getenv("AZURE_API_VERSION"),            "api_base": os.getenv("AZURE_API_BASE")        },        "tpm": 240000,        "rpm": 1800    },     {        "model_name": "azure/gpt-3.5-turbo", # openai model name         "litellm_params": { # params for litellm completion/embedding call             "model": "azure/chatgpt-functioncalling",             "api_key": "bad-key",            "api_version": os.getenv("AZURE_API_VERSION"),            "api_base": os.getenv("AZURE_API_BASE")        },        "tpm": 240000,        "rpm": 1800    },     {        "model_name": "gpt-3.5-turbo", # openai model name         "litellm_params": { # params for litellm completion/embedding call             "model": "gpt-3.5-turbo",             "api_key": os.getenv("OPENAI_API_KEY"),        },        "tpm": 1000000,        "rpm": 9000    },    {        "model_name": "gpt-3.5-turbo-16k", # openai model name         "litellm_params": { # params for litellm completion/embedding call             "model": "gpt-3.5-turbo-16k",             "api_key": os.getenv("OPENAI_API_KEY"),        },        "tpm": 1000000,        "rpm": 9000    }]router = Router(model_list=model_list,                 fallbacks=[{"azure/gpt-3.5-turbo": ["gpt-3.5-turbo"]}],                 default_fallbacks=["gpt-3.5-turbo-16k"],                context_window_fallbacks=[{"azure/gpt-3.5-turbo-context-fallback": ["gpt-3.5-turbo-16k"]}, {"gpt-3.5-turbo": ["gpt-3.5-turbo-16k"]}],                set_verbose=True)user_message = "Hello, whats the weather in San Francisco??"messages = [{"content": user_message, "role": "user"}]# normal fallback call response = router.completion(model="azure/gpt-3.5-turbo", messages=messages)# context window fallback callresponse = router.completion(model="azure/gpt-3.5-turbo-context-fallback", messages=messages)print(f"response: {response}")Caching​In production, we recommend using a Redis cache. For quickly testing things locally, we also support simple in-memory caching. In-memory Cacherouter = Router(model_list=model_list,                 cache_responses=True)print(response)Redis Cacherouter = Router(model_list=model_list,                 redis_host=os.getenv("REDIS_HOST"),                 redis_password=os.getenv("REDIS_PASSWORD"),                 redis_port=os.getenv("REDIS_PORT"),                cache_responses=True)print(response)Pass in Redis URL, additional kwargs router = Router(model_list: Optional[list] = None,                 ## CACHING ##                  redis_url=os.getenv("REDIS_URL")",                 cache_kwargs= {}, # additional kwargs to pass to RedisCache (see caching.py)                 cache_responses=True)Pre-Call Checks (Context Window, EU-Regions)​Enable pre-call checks to filter out:deployments with context window limit < messages for a call.deployments outside of eu-regionSDKProxy1. Enable pre-call checksfrom litellm import Router # ...router = Router(model_list=model_list, enable_pre_call_checks=True) # 👈 Set to True2. Set Model ListFor context window checks on azure deployments, set the base model. Pick the base model from this list, all the azure models start with azure/. For 'eu-region' filtering, Set 'region_name' of deployment. Note: We automatically infer region_name for Vertex AI, Bedrock, and IBM WatsonxAI based on your litellm params. For Azure, set litellm.enable_preview = True.See Codemodel_list = [            {                "model_name": "gpt-3.5-turbo", # model group name                "litellm_params": {  # params for litellm completion/embedding call                    "model": "azure/chatgpt-v-2",                    "api_key": os.getenv("AZURE_API_KEY"),                    "api_version": os.getenv("AZURE_API_VERSION"),                    "api_base": os.getenv("AZURE_API_BASE"),                    "region_name": "eu" # 👈 SET 'EU' REGION NAME                    "base_model": "azure/gpt-35-turbo", # 👈 (Azure-only) SET BASE MODEL                },            },            {                "model_name": "gpt-3.5-turbo", # model group name                "litellm_params": {  # params for litellm completion/embedding call                    "model": "gpt-3.5-turbo-1106",                    "api_key": os.getenv("OPENAI_API_KEY"),                },            },            {                "model_name": "gemini-pro",                "litellm_params: {                    "model": "vertex_ai/gemini-pro-1.5",                     "vertex_project": "adroit-crow-1234",                    "vertex_location": "us-east1" # 👈 AUTOMATICALLY INFERS 'region_name'                }            }        ]router = Router(model_list=model_list, enable_pre_call_checks=True) 3. Test it!Context Window CheckEU Region Check"""- Give a gpt-3.5-turbo model group with different context windows (4k vs. 16k)- Send a 5k prompt- Assert it works"""from litellm import Routerimport osmodel_list = [    {        "model_name": "gpt-3.5-turbo",  # model group name        "litellm_params": {  # params for litellm completion/embedding call            "model": "azure/chatgpt-v-2",            "api_key": os.getenv("AZURE_API_KEY"),            "api_version": os.getenv("AZURE_API_VERSION"),            "api_base": os.getenv("AZURE_API_BASE"),            "base_model": "azure/gpt-35-turbo",        },        "model_info": {            "base_model": "azure/gpt-35-turbo",         }    },    {        "model_name": "gpt-3.5-turbo",  # model group name        "litellm_params": {  # params for litellm completion/embedding call            "model": "gpt-3.5-turbo-1106",            "api_key": os.getenv("OPENAI_API_KEY"),        },    },]router = Router(model_list=model_list, enable_pre_call_checks=True) text = "What is the meaning of 42?" * 5000response = router.completion(    model="gpt-3.5-turbo",    messages=[        {"role": "system", "content": text},        {"role": "user", "content": "Who was Alexander?"},    ],)print(f"response: {response}")"""- Give 2 gpt-3.5-turbo deployments, in eu + non-eu regions- Make a call- Assert it picks the eu-region model"""from litellm import Routerimport osmodel_list = [    {        "model_name": "gpt-3.5-turbo",  # model group name        "litellm_params": {  # params for litellm completion/embedding call            "model": "azure/chatgpt-v-2",            "api_key": os.getenv("AZURE_API_KEY"),            "api_version": os.getenv("AZURE_API_VERSION"),            "api_base": os.getenv("AZURE_API_BASE"),            "region_name": "eu"        },        "model_info": {            "id": "1"        }    },    {        "model_name": "gpt-3.5-turbo",  # model group name        "litellm_params": {  # params for litellm completion/embedding call            "model": "gpt-3.5-turbo-1106",            "api_key": os.getenv("OPENAI_API_KEY"),        },        "model_info": {            "id": "2"        }    },]router = Router(model_list=model_list, enable_pre_call_checks=True) response = router.completion(    model="gpt-3.5-turbo",    messages=[{"role": "user", "content": "Who was Alexander?"}],)print(f"response: {response}")print(f"response id: {response._hidden_params['model_id']}")infoGo here for how to do this on the proxyCaching across model groups​If you want to cache across 2 different model groups (e.g. azure deployments, and openai), use caching groups. import litellm, asyncio, timefrom litellm import Router # set os envos.environ["OPENAI_API_KEY"] = ""os.environ["AZURE_API_KEY"] = ""os.environ["AZURE_API_BASE"] = ""os.environ["AZURE_API_VERSION"] = ""async def test_acompletion_caching_on_router_caching_groups():     # tests acompletion + caching on router     try:        litellm.set_verbose = True        model_list = [            {                "model_name": "openai-gpt-3.5-turbo",                "litellm_params": {                    "model": "gpt-3.5-turbo-0613",                    "api_key": os.getenv("OPENAI_API_KEY"),                },            },            {                "model_name": "azure-gpt-3.5-turbo",                "litellm_params": {                    "model": "azure/chatgpt-v-2",                    "api_key": os.getenv("AZURE_API_KEY"),                    "api_base": os.getenv("AZURE_API_BASE"),                    "api_version": os.getenv("AZURE_API_VERSION")                },            }        ]        messages = [            {"role": "user", "content": f"write a one sentence poem {time.time()}?"}        ]        start_time = time.time()        router = Router(model_list=model_list,                 cache_responses=True,                 caching_groups=[("openai-gpt-3.5-turbo", "azure-gpt-3.5-turbo")])        response1 = await router.acompletion(model="openai-gpt-3.5-turbo", messages=messages, temperature=1)        print(f"response1: {response1}")        await asyncio.sleep(1) # add cache is async, async sleep for cache to get set        response2 = await router.acompletion(model="azure-gpt-3.5-turbo", messages=messages, temperature=1)        assert response1.id == response2.id        assert len(response1.choices[0].message.content) > 0        assert response1.choices[0].message.content == response2.choices[0].message.content    except Exception as e:        traceback.print_exc()asyncio.run(test_acompletion_caching_on_router_caching_groups())Alerting 🚨​Send alerts to slack / your webhook url for the following eventsLLM API ExceptionsSlow LLM ResponsesGet a slack webhook url from https://api.slack.com/messaging/webhooksUsage​Initialize an AlertingConfig and pass it to litellm.Router. The following code will trigger an alert because api_key=bad-key which is invalidfrom litellm.router import AlertingConfigimport litellmimport osrouter = litellm.Router(    model_list=[        {            "model_name": "gpt-3.5-turbo",            "litellm_params": {                "model": "gpt-3.5-turbo",                "api_key": "bad_key",            },        }    ],    alerting_config= AlertingConfig(        alerting_threshold=10,                        # threshold for slow / hanging llm responses (in seconds). Defaults to 300 seconds        webhook_url= os.getenv("SLACK_WEBHOOK_URL")   # webhook you want to send alerts to    ),)try:    await router.acompletion(        model="gpt-3.5-turbo",        messages=[{"role": "user", "content": "Hey, how's it going?"}],    )except:    passTrack cost for Azure Deployments​Problem: Azure returns gpt-4 in the response when azure/gpt-4-1106-preview is used. This leads to inaccurate cost trackingSolution ✅ :  Set model_info["base_model"] on your router init so litellm uses the correct model for calculating azure costStep 1. Router Setupfrom litellm import Routermodel_list = [    { # list of model deployments         "model_name": "gpt-4-preview", # model alias         "litellm_params": { # params for litellm completion/embedding call             "model": "azure/chatgpt-v-2", # actual model name            "api_key": os.getenv("AZURE_API_KEY"),            "api_version": os.getenv("AZURE_API_VERSION"),            "api_base": os.getenv("AZURE_API_BASE")        },        "model_info": {            "base_model": "azure/gpt-4-1106-preview" # azure/gpt-4-1106-preview will be used for cost tracking, ensure this exists in litellm model_prices_and_context_window.json        }    },     {        "model_name": "gpt-4-32k",         "litellm_params": { # params for litellm completion/embedding call             "model": "azure/chatgpt-functioncalling",             "api_key": os.getenv("AZURE_API_KEY"),            "api_version": os.getenv("AZURE_API_VERSION"),            "api_base": os.getenv("AZURE_API_BASE")        },        "model_info": {            "base_model": "azure/gpt-4-32k" # azure/gpt-4-32k will be used for cost tracking, ensure this exists in litellm model_prices_and_context_window.json        }    }]router = Router(model_list=model_list)Step 2. Access response_cost in the custom callback, litellm calculates the response cost for youimport litellmfrom litellm.integrations.custom_logger import CustomLoggerclass MyCustomHandler(CustomLogger):            def log_success_event(self, kwargs, response_obj, start_time, end_time):         print(f"On Success")        response_cost = kwargs.get("response_cost")        print("response_cost=", response_cost)customHandler = MyCustomHandler()litellm.callbacks = [customHandler]# router completion callresponse = router.completion(    model="gpt-4-32k",     messages=[{ "role": "user", "content": "Hi who are you"}])Default litellm.completion/embedding params​You can also set default params for litellm completion/embedding calls. Here's how to do that: from litellm import Routerfallback_dict = {"gpt-3.5-turbo": "gpt-3.5-turbo-16k"}router = Router(model_list=model_list,                 default_litellm_params={"context_window_fallback_dict": fallback_dict})user_message = "Hello, whats the weather in San Francisco??"messages = [{"content": user_message, "role": "user"}]# normal call response = router.completion(model="gpt-3.5-turbo", messages=messages)print(f"response: {response}")Custom Callbacks - Track API Key, API Endpoint, Model Used​If you need to track the api_key, api endpoint, model, custom_llm_provider used for each completion call, you can setup a custom callback Usage​import litellmfrom litellm.integrations.custom_logger import CustomLoggerclass MyCustomHandler(CustomLogger):            def log_success_event(self, kwargs, response_obj, start_time, end_time):         print(f"On Success")        print("kwargs=", kwargs)        litellm_params= kwargs.get("litellm_params")        api_key = litellm_params.get("api_key")        api_base = litellm_params.get("api_base")        custom_llm_provider= litellm_params.get("custom_llm_provider")        response_cost = kwargs.get("response_cost")        # print the values        print("api_key=", api_key)        print("api_base=", api_base)        print("custom_llm_provider=", custom_llm_provider)        print("response_cost=", response_cost)    def log_failure_event(self, kwargs, response_obj, start_time, end_time):         print(f"On Failure")        print("kwargs=")customHandler = MyCustomHandler()litellm.callbacks = [customHandler]# Init Routerrouter = Router(model_list=model_list, routing_strategy="simple-shuffle")# router completion callresponse = router.completion(    model="gpt-3.5-turbo",     messages=[{ "role": "user", "content": "Hi who are you"}])Deploy Router​If you want a server to load balance across different LLM APIs, use our OpenAI Proxy ServerInit Params for the litellm.Router​def __init__(    model_list: Optional[list] = None,        ## CACHING ##    redis_url: Optional[str] = None,    redis_host: Optional[str] = None,    redis_port: Optional[int] = None,    redis_password: Optional[str] = None,    cache_responses: Optional[bool] = False,    cache_kwargs: dict = {},  # additional kwargs to pass to RedisCache (see caching.py)    caching_groups: Optional[        List[tuple]    ] = None,  # if you want to cache across model groups    client_ttl: int = 3600,  # ttl for cached clients - will re-initialize after this time in seconds    ## RELIABILITY ##    num_retries: int = 0,    timeout: Optional[float] = None,    default_litellm_params={},  # default params for Router.chat.completion.create    fallbacks: Optional[List] = None,    default_fallbacks: Optional[List] = None    allowed_fails: Optional[int] = None, # Number of times a deployment can failbefore being added to cooldown    cooldown_time: float = 1,  # (seconds) time to cooldown a deployment after failure    context_window_fallbacks: Optional[List] = None,    model_group_alias: Optional[dict] = {},    retry_after: int = 0,  # (min) time to wait before retrying a failed request    routing_strategy: Literal[        "simple-shuffle",        "least-busy",        "usage-based-routing",        "latency-based-routing",        "cost-based-routing",    ] = "simple-shuffle",    ## DEBUGGING ##    set_verbose: bool = False,  # set this to True for seeing logs    debug_level: Literal["DEBUG", "INFO"] = "INFO", # set this to "DEBUG" for detailed debugging):Debugging Router​Basic Debugging​Set Router(set_verbose=True)from litellm import Routerrouter = Router(    model_list=model_list,    set_verbose=True)Detailed Debugging​Set Router(set_verbose=True,debug_level="DEBUG")from litellm import Routerrouter = Router(    model_list=model_list,    set_verbose=True,    debug_level="DEBUG"  # defaults to INFO)Very Detailed Debugging​Set litellm.set_verbose=True and Router(set_verbose=True,debug_level="DEBUG")from litellm import Routerimport litellmlitellm.set_verbose = Truerouter = Router(    model_list=model_list,    set_verbose=True,    debug_level="DEBUG"  # defaults to INFO)PreviousCustom Pricing - Sagemaker, etc.Next[BETA] Request PrioritizationLoad BalancingQuick StartAvailable EndpointsAdvanced - Routing StrategiesSet Time WindowSet Lowest Latency BufferBasic ReliabilityMax Parallel Requests (ASYNC)TimeoutsCooldownsRetriesAdvanced: Custom Retries, Cooldowns based on Error TypeFallbacksCachingPre-Call Checks (Context Window, EU-Regions)Caching across model groupsAlerting 🚨Track cost for Azure DeploymentsCustom Callbacks - Track API Key, API Endpoint, Model UsedUsageDeploy RouterInit Params for the litellm.RouterDebugging RouterBasic DebuggingDetailed DebuggingVery Detailed Debugging

Logging & ObservabilityCallbacksOn this pageCallbacksUse Callbacks to send Output Data to Posthog, Sentry etc​liteLLM provides input_callbacks, success_callbacks and failure_callbacks, making it easy for you to send data to a particular provider depending on the status of your responses.liteLLM supports:Custom Callback FunctionsLunaryLangfuseHeliconeTraceloopAthinaSentryPostHogSlackQuick Start​from litellm import completion# set callbackslitellm.input_callback=["sentry"] # for sentry breadcrumbing - logs the input being sent to the apilitellm.success_callback=["posthog", "helicone", "langfuse", "lunary", "athina"]litellm.failure_callback=["sentry", "lunary", "langfuse"]## set env variablesos.environ['SENTRY_DSN'], os.environ['SENTRY_API_TRACE_RATE']= ""os.environ['POSTHOG_API_KEY'], os.environ['POSTHOG_API_URL'] = "api-key", "api-url"os.environ["HELICONE_API_KEY"] = ""os.environ["TRACELOOP_API_KEY"] = ""os.environ["LUNARY_PUBLIC_KEY"] = ""os.environ["ATHINA_API_KEY"] = ""os.environ["LANGFUSE_PUBLIC_KEY"] = ""os.environ["LANGFUSE_SECRET_KEY"] = ""os.environ["LANGFUSE_HOST"] = ""response = completion(model="gpt-3.5-turbo", messages=messages)PreviousLocal DebuggingNextCustom CallbacksUse Callbacks to send Output Data to Posthog, Sentry etcQuick Start

💥 OpenAI Proxy Server🔑 Virtual KeysOn this page🔑 Virtual KeysTrack Spend, and control model access via virtual keys for the proxyinfo🔑 UI to Generate, Edit, Delete Keys (with SSO)Deploy LiteLLM Proxy with Key ManagementDockerfile.database for LiteLLM Proxy + Key ManagementSetup​Requirements: Need a postgres database (e.g. Supabase, Neon, etc)Set DATABASE_URL=postgresql://<user>:<password>@<host>:<port>/<dbname> in your env Set a master key, this is your Proxy Admin key - you can use this to create other keys (🚨 must start with sk-). Set on config.yaml set your master key under general_settings:master_key, example below Set env variable set LITELLM_MASTER_KEY(the proxy Dockerfile checks if the DATABASE_URL is set and then intializes the DB connection)export DATABASE_URL=postgresql://<user>:<password>@<host>:<port>/<dbname>You can then generate keys by hitting the /key/generate endpoint.See codeStep 1: Save postgres db urlmodel_list:  - model_name: gpt-4    litellm_params:        model: ollama/llama2  - model_name: gpt-3.5-turbo    litellm_params:        model: ollama/llama2general_settings:   master_key: sk-1234   database_url: "postgresql://<user>:<password>@<host>:<port>/<dbname>" # 👈 KEY CHANGEStep 2: Start litellmlitellm --config /path/to/config.yamlStep 3: Generate keyscurl 'http://0.0.0.0:4000/key/generate' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4"], "metadata": {"user": "ishaan@berri.ai"}}'Advanced - Spend Tracking​Get spend per:key - via /key/info Swaggeruser - via /user/info Swaggerteam - via /team/info Swagger ⏳ end-users - via /end_user/info - Comment on this issue for end-user cost trackingHow is it calculated?The cost per model is stored here and calculated by the completion_cost function.How is it tracking?Spend is automatically tracked for the key in the "LiteLLM_VerificationTokenTable". If the key has an attached 'user_id' or 'team_id', the spend for that user is tracked in the "LiteLLM_UserTable", and team in the "LiteLLM_TeamTable".Key SpendUser SpendTeam SpendYou can get spend for a key by using the /key/info endpoint. curl 'http://0.0.0.0:4000/key/info?key=<user-key>' \     -X GET \     -H 'Authorization: Bearer <your-master-key>'This is automatically updated (in USD) when calls are made to /completions, /chat/completions, /embeddings using litellm's completion_cost() function. See Code. Sample response{    "key": "sk-tXL0wt5-lOOVK9sfY2UacA",    "info": {        "token": "sk-tXL0wt5-lOOVK9sfY2UacA",        "spend": 0.0001065, # 👈 SPEND        "expires": "2023-11-24T23:19:11.131000Z",        "models": [            "gpt-3.5-turbo",            "gpt-4",            "claude-2"        ],        "aliases": {            "mistral-7b": "gpt-3.5-turbo"        },        "config": {}    }}1. Create a usercurl --location 'http://localhost:4000/user/new' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{user_email: "krrish@berri.ai"}' Expected Response{    ...    "expires": "2023-12-22T09:53:13.861000Z",    "user_id": "my-unique-id", # 👈 unique id    "max_budget": 0.0}2. Create a key for that usercurl 'http://0.0.0.0:4000/key/generate' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4"], "user_id": "my-unique-id"}'Returns a key - sk-....3. See spend for usercurl 'http://0.0.0.0:4000/user/info?user_id=my-unique-id' \     -X GET \     -H 'Authorization: Bearer <your-master-key>'Expected Response{  ...  "spend": 0 # 👈 SPEND}Use teams, if you want keys to be owned by multiple people (e.g. for a production app).1. Create a teamcurl --location 'http://localhost:4000/team/new' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{"team_alias": "my-awesome-team"}' Expected Response{    ...    "expires": "2023-12-22T09:53:13.861000Z",    "team_id": "my-unique-id", # 👈 unique id    "max_budget": 0.0}2. Create a key for that teamcurl 'http://0.0.0.0:4000/key/generate' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{"models": ["gpt-3.5-turbo", "gpt-4"], "team_id": "my-unique-id"}'Returns a key - sk-....3. See spend for teamcurl 'http://0.0.0.0:4000/team/info?team_id=my-unique-id' \     -X GET \     -H 'Authorization: Bearer <your-master-key>'Expected Response{  ...  "spend": 0 # 👈 SPEND}Advanced - Model Access​Restrict models by team_id​litellm-dev can only access azure-gpt-3.51. Create a team via /team/newcurl --location 'http://localhost:4000/team/new' \--header 'Authorization: Bearer <your-master-key>' \--header 'Content-Type: application/json' \--data-raw '{  "team_alias": "litellm-dev",  "models": ["azure-gpt-3.5"]}' # returns {...,"team_id": "my-unique-id"}2. Create a key for teamcurl --location 'http://localhost:4000/key/generate' \--header 'Authorization: Bearer sk-1234' \--header 'Content-Type: application/json' \--data-raw '{"team_id": "my-unique-id"}'3. Test itcurl --location 'http://0.0.0.0:4000/chat/completions' \    --header 'Content-Type: application/json' \    --header 'Authorization: Bearer sk-qo992IjKOC2CHKZGRoJIGA' \    --data '{        "model": "BEDROCK_GROUP",        "messages": [            {                "role": "user",                "content": "hi"            }        ]    }'{"error":{"message":"Invalid model for team litellm-dev: BEDROCK_GROUP.  Valid models for team are: ['azure-gpt-3.5']\n\n\nTraceback (most recent call last):\n  File \"/Users/ishaanjaffer/Github/litellm/litellm/proxy/proxy_server.py\", line 2298, in chat_completion\n    _is_valid_team_configs(\n  File \"/Users/ishaanjaffer/Github/litellm/litellm/proxy/utils.py\", line 1296, in _is_valid_team_configs\n    raise Exception(\nException: Invalid model for team litellm-dev: BEDROCK_GROUP.  Valid models for team are: ['azure-gpt-3.5']\n\n","type":"None","param":"None","code":500}}%            Model Aliases​If a user is expected to use a given model (i.e. gpt3-5), and you want to:try to upgrade the request (i.e. GPT4)or downgrade it (i.e. Mistral)OR rotate the API KEY (i.e. open AI)OR access the same model through different end points (i.e. openAI vs openrouter vs Azure)Here's how you can do that: Step 1: Create a model group in config.yaml (save model name, api keys, etc.)model_list:  - model_name: my-free-tier    litellm_params:        model: huggingface/HuggingFaceH4/zephyr-7b-beta        api_base: http://0.0.0.0:8001  - model_name: my-free-tier    litellm_params:        model: huggingface/HuggingFaceH4/zephyr-7b-beta        api_base: http://0.0.0.0:8002  - model_name: my-free-tier    litellm_params:        model: huggingface/HuggingFaceH4/zephyr-7b-beta        api_base: http://0.0.0.0:8003    - model_name: my-paid-tier    litellm_params:        model: gpt-4        api_key: my-api-keyStep 2: Generate a user key - enabling them access to specific models, custom model aliases, etc.curl -X POST "https://0.0.0.0:4000/key/generate" \-H "Authorization: Bearer <your-master-key>" \-H "Content-Type: application/json" \-d '{    "models": ["my-free-tier"],     "aliases": {"gpt-3.5-turbo": "my-free-tier"},     "duration": "30min"}'How to upgrade / downgrade request? Change the alias mappingHow are routing between diff keys/api bases done? litellm handles this by shuffling between different models in the model list with the same model_name. See CodeGrant Access to new model​Use model access groups to give users access to select models, and add new ones to it over time (e.g. mistral, llama-2, etc.)Step 1. Assign model, access group in config.yamlmodel_list:  - model_name: text-embedding-ada-002    litellm_params:      model: azure/azure-embedding-model      api_base: "os.environ/AZURE_API_BASE"      api_key: "os.environ/AZURE_API_KEY"      api_version: "2023-07-01-preview"    model_info:      access_groups: ["beta-models"] # 👈 Model Access GroupStep 2. Create key with access groupcurl --location 'http://localhost:4000/key/generate' \-H 'Authorization: Bearer <your-master-key>' \-H 'Content-Type: application/json' \-d '{"models": ["beta-models"], # 👈 Model Access Group            "max_budget": 0,}'Advanced - Custom Auth​You can now override the default api key auth.Here's how: 1. Create a custom auth file.​Make sure the response type follows the UserAPIKeyAuth pydantic object. This is used by for logging usage specific to that user key.from litellm.proxy._types import UserAPIKeyAuthasync def user_api_key_auth(request: Request, api_key: str) -> UserAPIKeyAuth:     try:         modified_master_key = "sk-my-master-key"        if api_key == modified_master_key:            return UserAPIKeyAuth(api_key=api_key)        raise Exception    except:         raise Exception2. Pass the filepath (relative to the config.yaml)​Pass the filepath to the config.yaml e.g. if they're both in the same dir - ./config.yaml and ./custom_auth.py, this is what it looks like:model_list:   - model_name: "openai-model"    litellm_params:       model: "gpt-3.5-turbo"litellm_settings:  drop_params: True  set_verbose: Truegeneral_settings:  custom_auth: custom_auth.user_api_key_authImplementation Code3. Start the proxy​$ litellm --config /path/to/config.yaml Custom /key/generate​If you need to add custom logic before generating a Proxy API Key (Example Validating team_id)1. Write a custom custom_generate_key_fn​The input to the custom_generate_key_fn function is a single parameter: data (Type: GenerateKeyRequest)The output of your custom_generate_key_fn should be a dictionary with the following structure{    "decision": False,    "message": "This violates LiteLLM Proxy Rules. No team id provided.",}decision (Type: bool): A boolean value indicating whether the key generation is allowed (True) or not (False).message (Type: str, Optional): An optional message providing additional information about the decision. This field is included when the decision is False.async def custom_generate_key_fn(data: GenerateKeyRequest)-> dict:        """        Asynchronous function for generating a key based on the input data.        Args:            data (GenerateKeyRequest): The input data for key generation.        Returns:            dict: A dictionary containing the decision and an optional message.            {                "decision": False,                "message": "This violates LiteLLM Proxy Rules. No team id provided.",            }        """                # decide if a key should be generated or not        print("using custom auth function!")        data_json = data.json()  # type: ignore        # Unpacking variables        team_id = data_json.get("team_id")        duration = data_json.get("duration")        models = data_json.get("models")        aliases = data_json.get("aliases")        config = data_json.get("config")        spend = data_json.get("spend")        user_id = data_json.get("user_id")        max_parallel_requests = data_json.get("max_parallel_requests")        metadata = data_json.get("metadata")        tpm_limit = data_json.get("tpm_limit")        rpm_limit = data_json.get("rpm_limit")        if team_id is not None and team_id == "litellm-core-infra@gmail.com":            # only team_id="litellm-core-infra@gmail.com" can make keys            return {                "decision": True,            }        else:            print("Failed custom auth")            return {                "decision": False,                "message": "This violates LiteLLM Proxy Rules. No team id provided.",            }2. Pass the filepath (relative to the config.yaml)​Pass the filepath to the config.yaml e.g. if they're both in the same dir - ./config.yaml and ./custom_auth.py, this is what it looks like:model_list:   - model_name: "openai-model"    litellm_params:       model: "gpt-3.5-turbo"litellm_settings:  drop_params: True  set_verbose: Truegeneral_settings:  custom_key_generate: custom_auth.custom_generate_key_fnUpperbound /key/generate params​Use this, if you need to set default upperbounds for max_budget, budget_duration or any key/generate param per key. Set litellm_settings:upperbound_key_generate_params:litellm_settings:  upperbound_key_generate_params:    max_budget: 100 # upperbound of $100, for all /key/generate requests    duration: "30d" # upperbound of 30 days for all /key/generate requests Expected Behavior Send a /key/generate request with max_budget=200Key will be created with max_budget=100 since 100 is the upper boundDefault /key/generate params​Use this, if you need to control the default max_budget or any key/generate param per key. When a /key/generate request does not specify max_budget, it will use the max_budget specified in default_key_generate_paramsSet litellm_settings:default_key_generate_params:litellm_settings:  default_key_generate_params:    max_budget: 1.5000    models: ["azure-gpt-3.5"]    duration:     # blank means `null`    metadata: {"setting":"default"}    team_id: "core-infra"Endpoints​Keys​👉 API REFERENCE DOCS​Users​👉 API REFERENCE DOCS​Teams​👉 API REFERENCE DOCS​Previous✨ Enterprise Features - Content Mod, SSO, Custom SwaggerNext🚨 Alerting / WebhooksSetupAdvanced - Spend TrackingAdvanced - Model AccessRestrict models by team_idModel AliasesGrant Access to new modelAdvanced - Custom AuthCustom /key/generateUpperbound /key/generate paramsDefault /key/generate paramsEndpointsKeysUsersTeams